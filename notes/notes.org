* General
** DONE Give each experiment its own resource folder
   CLOSED: [2020-10-12 Mon 09:51]
   - But maybe it's ok to use a communal one.
** DONE Split up the omnibus data files before data cleaning so that everything isn't rerun after every new import
   CLOSED: [2020-10-26 Mon 09:47]
** TODO Read "The allocation of resources within firms"
** Prolific
   - Don't recruit multiple versions of the same study at once
     - You can't set exclusions for studies that haven't been 'completed'
** TODO Open {drake} question about "skipping" static branching targets
** Mixed models
*** Subject variance = zero
    - Note, we get singularity even for minimal model for alignment 2
      - [[https://stats.stackexchange.com/a/112435][This]] suggests that you can keep as is, or even just do a normal
        regression in that case.
    - Turns out this is also the case for alignment 7
    - So do we just simulate the data with subject variance of zero?
    - [[https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022509.html][Here]] Bates suggests that variance of zero just suggests the model should
    be simplified 
    - Also:
    #+begin_quote
    It is much more likely that there is insufficient data to estimate the
    parameters in a model of this level of complexity.
    #+end_quote
    - According to [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/][this]], random effects...
    #+begin_quote
    are quite ‘data hungry’; requiring at least five ‘levels’ (groups) for a
    random intercept term to achieve robust estimates of variance (Gelman &
    Hill, 2007; Harrison, 2015). With <5 levels, the mixed model may not be able
    to estimate the among-population variance accurately. In this case, the
    variance estimate will either collapse to zero, making the model equivalent
    to an ordinary GLM (Gelman & Hill, 2007, p. 275) or be non-zero but
    incorrect if the small number of groups that were sampled are not
    representative of true distribution of means (Harrison, 2015).
    #+end_quote
    - But [[https://stats.stackexchange.com/a/466286][here]] Ben Bolker says it's fine to keep it even if fit is singular.
** TODO Work out why you can pass symbols for functions in drake, but not for arguments
** DONE Work out why sometimes drake re-runs when switching computers
   CLOSED: [2020-12-11 Fri 09:16]
   - Seems to be set off by the resources folders
   - I think because when you change computer the file path changes
   - Also happens to memos, because they have a path argument for ~knitr_in~
   - https://github.com/ropensci/drake/issues/1350
** Title
   - Resource allocation biases: data and semantics
   - Data vs. semantics: An investigation of resource allocation biases
   - Data vs. semantics: Decision-making biases in resource allocation
   - The influence of semantics over statistics in managerial risky choice and resource allocation
     - From thesis_doc
** Thesis logic
   - People treat projects one at a time.
     - Aggregation
   - If you force them to look at them together, alignment matters.
     - Alignment
   - The degree of alignment matters?
   - Nah that doesn't make sense, because the alignment there is about a different thing
   - How about:
     - People prefer case studies to data
       - Anecdotes
     - People treat projects one at a time.
       - Aggregation
     - If you force them to look at them together, alignment matters.
       - Alignmendt
   - Not good either.
   - What was the anecdotes link previously?
     - Can't find in meeting notes or thesis doc attempts
   - Also the link between aggregation and alignment is now weird because you
     didn't replicate the joint vs separate effects
   - Ah, maybe the link is: anecdotes work shows that people like case studies
     over projects, so let's show them single projects/this suggests that they
     won't like to aggregate.
   - Nah, I think what it is is that, assuming people will be met with low
     similarity situations, how much do they rely on anecdotes.
   - So it's:
     - People treat projects one at a time.
       - Aggregation
     - If you force them to look at them together, alignment matters.
       - Alignment
     - But, since most cases are low alignment, what influences their reliance
   on case studies?
   - Anecdotes
   - Another way to put it
     - when low similarity, they can therefore either use bad metrics, or go
   to case studies.
** Examiners
   - Current list
     - [[https://liberalarts.utexas.edu/psychology/faculty/markman][Art Markman]]
     - [[https://loewenstein.web.illinois.edu/][Jeffrey Loewenstein]] 
     - [[https://www.tuck.dartmouth.edu/faculty/faculty-directory/giovanni-gavetti][Giovanni Gavetti]]
     - [[https://mba.technion.ac.il/prof-ido-erev/][Ido Erev]]
     - [[https://www.mpib-berlin.mpg.de/staff/ralph-hertwig][Ralph Hertwig]]
     - [[https://ie.technion.ac.il/~yeldad/][Eldad Yechiam]]
     - [[http://elke-u-weber.com/][Elke Weber]]
     - [[https://www.qber.uni-kiel.de/de/team/alexander-klos][Alexander Klos]]
     - [[https://www.anderson.ucla.edu/faculty-and-research/management-and-organizations/faculty/fox][Craig Fox]]
     - [[https://www.wiwi.uni-muenster.de/fcm/de/das-fcm/lsf/team/thomas-langer][Thomas Langer]]
     - [[https://www.ru.nl/english/people/zeisberger-s/][Stefan Zeisberger]]
     - [[https://www.bwl.uni-mannheim.de/en/weber/][Martin Weber]]
     - [[https://uwaterloo.ca/psychology/people-profiles/derek-j-koehler][Derek Koehler]]
     - [[http://oriplonsky.com/][Ori Plonsky]]
     - [[https://scholar.harvard.edu/rabin/home][Matthew Rabin]]
     - [[https://www.wbs.ac.uk/about/person/daniel-read/][Daniel Read]]
     - [[https://www.kellogg.northwestern.edu/faculty/directory/bockenholt_ulf.aspx][Ulf Bockenholt]]
     - [[https://www8.gsb.columbia.edu/articles/authors/elizabeth-webb][Elizabeth Webb]]
     - [[https://www.anderson.ucla.edu/faculty-and-research/marketing/faculty/shu][Suzanne Shu]]
     - [[https://www.gsb.stanford.edu/faculty-research/faculty/baba-shiv][Baba Shiv]]
     - [[https://www.cmu.edu/dietrich/sds/people/faculty/george-loewenstein.html][George Loewenstein]]
     - [[https://www.sdabocconi.it/en/faculty/david-bardolet][David Bardolet]]
   - Long list could include anyone on the reference list
   - But:
     - Probably can't be too close to supervisors
     - Should have done something similar to the thesis
     - Should have something to do with business/economics/organisation
   - Probably not that connected to business
     - [[https://www.mpib-berlin.mpg.de/staff/ralph-hertwig][Ralph Hertwig]]
     - [[http://elke-u-weber.com/][Elke Weber]]
   - Probably too big
     - [[https://liberalarts.utexas.edu/psychology/faculty/markman][Art Markman]]
     - [[https://scholar.harvard.edu/rabin/home][Matthew Rabin]]
     - [[https://www.cmu.edu/dietrich/sds/people/faculty/george-loewenstein.html][George Loewenstein]]
   - Maybe too connected
     - [[https://www.anderson.ucla.edu/faculty-and-research/management-and-organizations/faculty/fox][Craig Fox]]
     - [[https://www.sdabocconi.it/en/faculty/david-bardolet][David Bardolet]]
   - Revised list
     - [[https://loewenstein.web.illinois.edu/][Jeffrey Loewenstein]] 
     - [[https://www.tuck.dartmouth.edu/faculty/faculty-directory/giovanni-gavetti][Giovanni Gavetti]]
     - [[https://mba.technion.ac.il/prof-ido-erev/][Ido Erev]]
     - [[https://ie.technion.ac.il/~yeldad/][Eldad Yechiam]]
     - [[https://www.qber.uni-kiel.de/de/team/alexander-klos][Alexander Klos]]
     - [[https://www.wiwi.uni-muenster.de/fcm/de/das-fcm/lsf/team/thomas-langer][Thomas Langer]]
     - [[https://www.ru.nl/english/people/zeisberger-s/][Stefan Zeisberger]]
     - [[https://www.bwl.uni-mannheim.de/en/weber/][Martin Weber]]
     - [[http://oriplonsky.com/][Ori Plonsky]]
     - [[https://www.wbs.ac.uk/about/person/daniel-read/][Daniel Read]]
     - [[https://www.kellogg.northwestern.edu/faculty/directory/bockenholt_ulf.aspx][Ulf Bockenholt]]
     - [[https://www8.gsb.columbia.edu/articles/authors/elizabeth-webb][Elizabeth Webb]]
     - [[https://www.anderson.ucla.edu/faculty-and-research/marketing/faculty/shu][Suzanne Shu]]
     - [[https://www.gsb.stanford.edu/faculty-research/faculty/baba-shiv][Baba Shiv]]
   - The talk by Derek Wyman clarifies conflicts of interest
   #+begin_quote
   
   “...actual, potential or perceived conflicts of interest”

   Examiner – Supervisor:

   – circumstances in which the supervisors(s) and the examiner were co-applicants
   on grant proposals, have co-published or worked in the same academic unit within
   the last 5 years.

   A person cannot be an examiner if they
   – have had involvement in the student’s research, or are a co-author on any part of the work;
   – have a past or current close personal relationship with the student or supervisor;
   – have had substantial contact with the student or supervisor in any other circumstance which might jeopardise the independence, or the perceived independence of the examination;
   – have been a research student of the supervisor within the last ten years; or have supervised the student at any time
   #+end_quote
   - So no Markman, Rabin, Bardolet
   - But Fox is fine, because not within five years
   - Updated list:
     - [[https://www.anderson.ucla.edu/faculty-and-research/management-and-organizations/faculty/fox][Craig Fox]]
     - [[https://www.cmu.edu/dietrich/sds/people/faculty/george-loewenstein.html][George Loewenstein]]
     - [[https://loewenstein.web.illinois.edu/][Jeffrey Loewenstein]] 
     - [[https://www.tuck.dartmouth.edu/faculty/faculty-directory/giovanni-gavetti][Giovanni Gavetti]]
     - [[https://mba.technion.ac.il/prof-ido-erev/][Ido Erev]]
     - [[https://ie.technion.ac.il/~yeldad/][Eldad Yechiam]]
     - [[https://www.qber.uni-kiel.de/de/team/alexander-klos][Alexander Klos]]
     - [[https://www.wiwi.uni-muenster.de/fcm/de/das-fcm/lsf/team/thomas-langer][Thomas Langer]]
     - [[https://www.ru.nl/english/people/zeisberger-s/][Stefan Zeisberger]]
     - [[https://www.bwl.uni-mannheim.de/en/weber/][Martin Weber]]
     - [[http://oriplonsky.com/][Ori Plonsky]]
     - [[https://www.wbs.ac.uk/about/person/daniel-read/][Daniel Read]]
     - [[https://www.kellogg.northwestern.edu/faculty/directory/bockenholt_ulf.aspx][Ulf Bockenholt]]
     - [[https://www8.gsb.columbia.edu/articles/authors/elizabeth-webb][Elizabeth Webb]]
     - [[https://www.anderson.ucla.edu/faculty-and-research/marketing/faculty/shu][Suzanne Shu]]
     - [[https://www.gsb.stanford.edu/faculty-research/faculty/baba-shiv][Baba Shiv]]
** TODO Add aggregation 1 to drake plan
** TODO Add alignment 1 to drake plan
** TODO Add alignment 3 to drake plan
** TODO Add alignment 4 to drake plan
** TODO Add alignment 5 to drake plan
** TODO Add alignment 6 to drake plan
** TODO Introduction chapter draft
** TODO Conclusion chapter draft
** TODO Aggregation chapter draft
** TODO Alignment chapter draft
** TODO Anecdotes chapter draft
   - State "TODO"       from              [2020-12-07 Mon 11:27]
* Anecdotes
** Conceptual discussion
*** Notes from original talk
    #+begin_quote
    - what the normative account is
    - can take business studies: people are case-based reasoners
    - something seems blunt about just using the brute statistics without distinguishing the relevance
    - part of what asking: can you distinguish anecdotal based thinking and case based
    - might be justifying more normally than marginal statistics
    - Micah: in the case of business specifically: when this might be relevant?
      - are these the right features
      - if seeing overall similarity that's not relevant...
    - robert: don't think agrees on last point
      - whole rage with precision medicine:
      - let's say identical twin got benefit of vitamin
      - i should take what they say more than random anecdote
    - micah: work in progress is thinking about normative things
      - easy to point when it's normative, e.g. ,vaccination
      - but other situations might not be
      - not saying it's always a bad thing
      - can check if people are being selective
        - might take expertise to figure out what the right dimesinos are
    - rob: if going from ignornace
      - maybe machine learning overall simialrity, then might be making good choices by looking at previous example
    - micah: if aggregating across and have similar case
      - strategy that's not horrible
      - there might be cases that when it's not a reasonable strategy people would still do it
      - can do more work that: situation where clearly similairty is important, and situation where it isn't importnat, and see if they're sensitive
      - real world decisions where might mean there is no adaptive thing like that
    - rob: if I was consultant, maybe similar cases might be sueful
      - influenced by overall similarity, but also relevant similarity
      - they'll ignore that we're both 6"1, when it's clear hight is not relevant to outcome

    - Gentner
      - can see rob's point: where oil well might be normative
      - with medical
        - might be able to do it
      - micah: in business clear how to find relevant dimension
        - in medicine: do these people use simialrity
    #+end_quote
*** Micah and Rob Goldstone emails
    #+begin_quote
    Hi Rob, I appreciate your question because we do need a normative analysis. I
    think you left at this point, but Nina Simms asked a similar question and I
    think I perhaps had a better answer the second time around, so wanted to get
    your thoughts. If say, the aggregated data was from a pool of similar cases
    (because there are plenty of cases within the more specific category that are
    successful) and we were clear about that, as opposed to the current experiment
    where we were more vague about the nature of the aggregated pool, would that be
    a better test-case to see if someone was suboptimally relying on the one case
    they just happen to have the details for?

    I agree generally, that a normative case needs to be developed regardless. But
    how do you think this context would fit into the norm?

    thanks, Micah
    #+end_quote
   
    #+begin_quote
    Micah (and Shir),

    yeah, I was thinking about this kind of thing too. Along these
    lines, I was thinking of setting up a situation where it was clear that there
    were descriptions/reviews of each of the cases prepared ahead of time, and you
    are just randomly sampling from one of these descriptions and showing it to
    participants. I think in that situation there’s not any more information
    provided by the anecdote, and so it shouldn’t be used (unless the content of the
    review makes it clear that the person is thinking like, or is like, the
    participant). Like your oil/microchip example, the sampling could also be said
    to be biased toward neutral or similar-to-self cases. Normatively I think people
    should be more influenced in the latter case, and I’m guessing that’s what you
    would find, based on your oil/microchip results.

    Neat stuff!
    Rob
    #+end_quote

    #+begin_quote
    Thanks Rob! I think I'll also more closely match the business and health domains
    as well. My intuition is that people are more likely to use irrelevant
    dimensions of similarity in the health domain because of a lack of a clear
    causal model (in which case, large samples to generalise from seems more
    normative). If it turns out people are more sophisticated and selective I'd be
    surprised, but actually pleasantly in terms of the implications.

    Best, Micah
    #+end_quote

    #+begin_quote
    Yeah, but also remember: if people don't have a lot causal knowledge, then the
    smart/sophisticated thing to do is use overall similarity between the known
    cases and one’s own situation in order to weight their outcomes. So, I think the
    normative model would use both overall similarity and dimension-specific
    similarity, tuning the use of each according to the quality of one’s knowledge
    about which dimensions are really important.
    #+end_quote

    #+begin_quote
    Meant to reply. I agree, that in the case that knowledge is lacking, overall
    similarity is a smart strategy. But, in the case where you have
    low-knowledge/overall similarity info of an individual case, but also know that
    individual case is an outlier from the overall trend from a larger sample, do
    you think that overall similarity should still be weighed heavily? It's
    specifically the low-knowledge strategy when in conflict with large samples that
    seem problematic to me.

    But does formal analysis or simulations suggest it is still a good strategy? Or
    is this highly dependent on the degree of variability in the large sample?

    thanks, your thoughts are very helpful!

    -mbg

    #+end_quote
*** Shir and Micah emails
    #+begin_quote
    Hey Micah,
 
    I put together everything I’ve got on Rob’s normative question (notes I took
    from your talk, and your email exchange with him); see below for reference. What
    do you think about this as a summary:
 
    - On the one hand, we know that aggregated data is more useful than an anecdote
      (from that dataset).
      - On the other hand, an anecdote that is identical (e.g., a twin) to the
        target situation use is more useful than aggregated data.
    - Therefore, aggregated data should be used when the anecdote is randomly drawn
      from a sample of cases from the aggregated dataset, and the anecdote should be
      used when it was picked out specifically for its causal similarities to the
      target situation.

    So I guess we can just tell participants that the anecdote comes from the
    dataset that the statistics are based on. In that case it should then be
    definitely normative to go with the statistics, right?
    #+end_quote
    #+begin_quote
    Ah I think we would want to further express that it is randomly sampled from
    the aggregate in some manner right? Don't we already say it comes from the
    set?

    I guess in terms of design, do we want a condition difference where it is more
    relatively better to use an anecdote vs. not? or perhaps something about the
    relevance of the specific dimensions of similarity vs. global similarity...

    hmm...though maybe we need to emphasise the randomness for now, but then just do
    this other issue later. Perhaps let's also hear what Dan says about this given
    he said he had an answer...
    #+end_quote
    #+begin_quote
    Yep sorry I forgot to add the randomly sample bit. Yes, we already say it comes
    from the set.

    I think for now just emphasising the randomness is a good idea, because we're
    already adding valence and I think one condition add at a time makes sense.

    We should also consider whether it's possible to put the thesis together without
    this experiment in case it drags on too much (e.g., I'm still working on it in
    Feb). At the moment it seems important so that the anecdotes chapter doesn't
    just have one experiment.
    #+end_quote
    #+begin_quote
    yeah; we could do one more experiment for the thesis, and still need more for a
    paper...

    and we probably want to embellish the random explanation a bit just to ensure
    they dont miss it.

    check out this line of research for instruction inspiration
    https://link.springer.com/article/10.3758/s13423-018-1562-2

    #+end_quote
*** Dealing with Rob's comments
    - @hayes2019
      - Diversity effect: generalisation occurs more from dissimilar sets than
    similar ones
    - Depends on whether participants are told that the items come from a
      random distribution (weak sampling) or were intentionally chosen (strong sampling)
      - [[https://osf.io/fpx9k/][OSF page]]
      - Strong sampling instruction excerpt:
        #+begin_quote
        Note that the instances were deliberately chosen to best illustrate the
        variety of living things that have the property.
        #+end_quote
      - Weak sampling instruction excerpt:
    #+begin_quote
    We asked a student to open a book on plants and animals at weak pages and note
    the first three living things they came across and whether or not those living
    things have the property in question.

    This means the information you receive may not be the most strong for making
    your judgment - by chance, the student will sometimes select very dissimilar
    items, and sometimes very similar ones.
    #+end_quote
*** New emails
    - Shir
    #+begin_quote
    I realise I forgot to update you after my call with Dan. We talked about the
    question of whether it is normative to choose statistics over an anecdote if
    the anecdote is similar to the target. Dan’s suggestion was to potentially
    add text that describes the “limit of similarity” in the sample. For
    instance, writing something like “the highest similarity rating of any of
    these case studies to various targets was 70%”. Supposedly this would mean
    that upon seeing a similar anecdote to the target project, participants
    should assume that at best the anecdote might be 70% similar, and not
    higher.
 
    So at the moment we have four ways of addressing the normative question in the
    experiment itself (i.e., apart from the justification in the paper):

    - Saying that the anecdote was randomly sampled from a pool of other projects
    - Saying that the pool is quite large (currently we say “thousands of projects”)
    - Using project-specific language when describing the failure/success of the
      company (e.g., the project failed because of a loss in stakeholder interest
      [project-specific] vs. the project failed because of problems with the soil in
      the region [would be true for all companies in the region])
    - Constraining range of possible similarity in the pool (described above)

    Thoughts on this? Specifically, I’m unsure of what to do with the third point
    (the “analysis” of why they failed/succeeded), because we used more general
    reasons in Experiment 1 (e.g., target is an oil well in Texas and the anecdote
    is an oil well in New Mexico, which we say had a “hydrocarbon shortage” in the
    region). That is, it seems that the more the reason has causal implications to
    the target (via any of the attributes, e.g., location, company structure, etc.)
    it increases the chances of it being useful. But also isn’t the point that there
    are sometimes flukes and natural variations (and therefore one should listen to
    the data)?
    #+end_quote
    - Micah
    #+begin_quote
    I have a few questions about % similarity. 1 is whether that undercuts our
    similarity manipulation? A big part of the effect from E1 was driven by
    perceived similarity, and what if the "70% similarity" reduces the perceived
    similarity between the two, and thus the effect? 2. it reads weirdly to me
    without then explaining how that is calculated. It could be good to e.g.,
    explain all the dimensions that go into it: (e.g., the amount invested, the
    management of the project). 3. Does quantifying the similarity actually imply
    they should use the anecdote? why else quantify the similarity besides to
    establish it's potential basis for induction? I realize this suggests the
    opposite effect of 1. Both seem possible to me...

    Either way for the %, the first 3 seem good; I think we need to make sure that
    the overall similarity of the high similarity is not directly support a causal
    explanation for failure, unless we can then argue they still should ignore it
    (but that certainly seems a harder argument). Of course, any individual may then
    construct a causal connection but that would then be a sort of explanation for
    why similarity has the effect, is how it elicits that causal connection. Thats
    different than explicitly supporting it.

    Or am I thinking of these in the wrong way?
    #+end_quote
    - Dan
    #+begin_quote
    it's converting similarity to probability weighting that concerns me most
    #+end_quote
*** First principles
    - People use anecdotes even in the presence of aggregated data.
      - e.g., medicine is taken more when description says "this medicine is 86%
        effective" than when it says "this medicine is 86% effective. Suzy took
        it and was ill."
      - Both cases imply that it didn't work for some people.
      - Maybe use percentage failed?
      - Not use analysis description at all?
    - How much should similarity affect this?
      - If twin, then should rely on that.
        - Because same causal structure
      - If very different, then should rely on data
      - So maybe it's a continuum, and it'll actually be hard to set it up as
        perfectly normative to always use anecdote
    - So maybe the question should be how /does/ similarity affect this?
      - And the question comes from:
        - On the one hand people like surface features over relational in
          retrieval
        - On the other hand are more sensitive to relations when directly
          comparing
      - Well that's not useful, because there's no retrieval.
      - We can just stick to the fact that the findings in the literature are
        mixed
      - 
** Experiment 1
*** DONE Summary
    CLOSED: [2020-12-04 Fri 15:14]
** Experiment 2
*** Design
    - IVs
      - Anecdote
      - Alignment
      - Polarity
    - DV
      - allocation
    - But the tricky thing is how we planned the new within-subjects design.
      - Anecdotes article Experiment 2 actually summarises it:
    #+begin_quote
    Experiment 2 was very similar to [Experiment 1], except that we added a
    within-subjects anecdote valence manipulation. Further, manipulated anecdote
    similarity within-subjects, in order to increase the experiment's power. All
    participants saw the statistics only condition, as it did not contain an
    anecdote, and therefore did not need to be manipulated between-subjects. As
    such, each participant saw five displays, with one statistics only condition,
    and four displays for either the anecdote only condition, or the statistics and
    anecdote condition. These four displays consisted of the similarity (low and
    high) $\times$ valence (negative and positive) conditions.
    #+end_quote
    - How many domains?
      - Each display has two, and then the anecdote is the same as one of them.
      - So we cycle between each of them as the anecdote/reference.
      - So we'd need an anecdote analysis for each.
      - Eight in total.
    - How do we structure this?
      - So the end result is a df with rows that indicate anecdote condition and
        whatever counterbalancing we need.
      - Each row has a nested df with five rows.
        - One for each within-subjects condition (statistics only, and anecdote
          condition f valence f similarity.
      - Each of those rows has a nested column with the actual HTML display
        - Which is either just a table for the statistics only, or a table and a
          paragraph for the anecdote condition.
        - But how do we do that? For alignment we sent off just the columns so
          that you can shuffle the columns.
        - But I guess here you only have two projects.
        - So maybe we can just counterbalance anyway without bloating the
          experiment file too much.
      - But the main source of counterbalancing will be the domains
        - Don't we actually need 10 domains, because the statistics only display
          also is displaying two projects.
        - Either way, we need  away of counterbalancing which domains go to
          which displays.
        - And which projects go to anecdote vs comparison.
        - Actually maybe it's not too bad.
          - Two/three anecdotes conditions (depending on if we do enhanced or not)
          - Five displays (five pairs of projects)
          - Two anecdote/comparison states (which of the pair is the anecdote
            and which is the comparison project)
          - 2/3 f 5 f 2
          - So 20/30 rows of five displays (of two projects each)
          - And double that if we counterbalance column order
            - But seems that we're better off to make a function for this like
              in alignment.
              - Takes rows as arguments, but also an anecdote argument.
      - So each domain needs different versions
        - low vs high intrinsic values
          - to be the anecdote or comparison, depending on valence condition
        - That's it really
        - Another way of putting it is that each needs
          - Anecdote description/analysis
          - High intrinsic features
          - Low intrinsic features
      - So we make a data frame with 10 rows for each domain
        - Each has the three components each needs as columns
        - Or two rows each because you can classify the two intrinsic features
          on a different column
          - And also because you always need an anecdote and one of the
            intrinsic features.
        - Then I guess we pair them
        - No, we do like in the alignment experiment
          - Create a latin nested column, and the unnest, so that each domain
            has one of the variation values.
        - But we should still pair them up.
          - So that you get five pairs.
          - And then you assign each pair a vector of 1-5 for their variation.
        - So that's for within-subjects allocation
          - Wait a second, each domain also needs two valence and two similarity condition.
        - So we'll have three main columns:
          - Anecdote (including description and analysis)
          - Intrinsic features (high and low)
          - Statistics (high, low, NA (but only for NA anecdote))
        - For anecdote, we'll have five rows:
          - High valence, high similarity
          - High valence, low similarity
          - Low valence, low similarity
          - Low valence, high similarity
          - NA
        - But the text of the anecdote depends on the intrinsic features
          - So I guess we actually mean "the descriptive components of the anecdote".
      - Also remember that all of this is just for the "low similarity"
        condition, because in high similarity they're comparing two projects
        from the same domain.
        - But the above technically already accounts for this because to do this
          we just need high and low value conditions
          | project | business | valence | similarity | statistics amount | anecdote | intrinsic |
          |---------+----------+---------+------------+-------------------+----------+-----------|
          | oil     | fuel co  | high    | low        | high              |          |           |
          | oil     | fuel co  | low     | low        | high              |          |           |
          | oil     | fuel co  | high    | high       | high              |          |           |
          | oil     | fuel co  | low     | high       | high              |          |           |
          | oil     | fuel co  | NA      | NA         | NA                |          |           |
          | oil     | refinera | high    | low        | low               |          |           |
          | oil     | refinera | low     | low        | low               |          |           |
          | oil     | refinera | high    | high       | low               |          |           |
          | oil     | refinera | low     | high       | low               |          |           |
          | oil     | refinera | NA      | NA         | NA                |          |           |
**** After reviewing Experiment 1
     - High and low similarity don't mean different business names
       - They have different business names regardless of similarity condition
       - They mean qualitative features that are similar (e.g., location) and
         quantitative values that are relevant
     - So each domain gets five components:
       1. Target project
       2. Anecdote - low valence high similarity
       3. Anecdote - high valence high similarity
       4. Anecdote - low valence low similarity
       5. Anecdote - high valence low similarity
          | project | project role | business | valence | similarity | analysis | features |
          |---------+--------------+----------+---------+------------+----------+----------|
          | oil     | target       | enfuel   | high    | high       | NA       | f        |
          | oil     | anecdote     | refinera | high    | high       | a1       | f1       |
          | oil     | target       | enfuel   | low     | high       | NA       | f        |
          | oil     | anecdote     | refinera | low     | high       | a2       | f2       |
          | oil     | target       | enfuel   | high    | low        | NA       | f        |
          | oil     | anecdote     | refinera | high    | low        | a3       | f3       |
          | oil     | target       | enfuel   | low     | low        | NA       | f        |
          | oil     | anecdote     | refinera | low     | low        | a4       | f4       |
     - And then when we filter by condition we always get the same target, and
       one of the four anecdotes
     - But I guess we have stats only
     - So we add a statistics column (we'll sketch just with one similarity condition)
       - But actually similarity doesn't mean anything in statistics only
       - Nor does valence.
       - So let's just add on
       - Anecdote only:
         | project | role     | business | valence | similarity | analysis | features | statistics |
         |---------+----------+----------+---------+------------+----------+----------+------------|
         | oil     | target   | enfuel   | high    | high       | NA       | f        | NA         |
         | oil     | anecdote | refinera | high    | high       | a1       | f1       | NA         |
         | oil     | target   | enfuel   | low     | high       | NA       | f        | NA         |
         | oil     | anecdote | refinera | low     | high       | a2       | f2       | NA         |
         | oil     | target   | enfuel   | high    | low        | NA       | f        | NA         |
         | oil     | anecdote | refinera | high    | low        | a3       | f3       | NA         |
         | oil     | target   | enfuel   | low     | low        | NA       | f        | NA         |
         | oil     | anecdote | refinera | low     | low        | a4       | f4       | NA         |
       - Anecdote + statistics:
         | project | role     | business | valence | similarity | analysis | features | statistics |
         |---------+----------+----------+---------+------------+----------+----------+------------|
         | oil     | target   | enfuel   | high    | high       | NA       | f        | high       |
         | oil     | anecdote | refinera | high    | high       | a1       | f1       | high       |
         | oil     | target   | enfuel   | low     | high       | NA       | f        | high       |
         | oil     | anecdote | refinera | low     | high       | a2       | f2       | high       |
         | oil     | target   | enfuel   | high    | low        | NA       | f        | high       |
         | oil     | anecdote | refinera | high    | low        | a3       | f3       | high       |
         | oil     | target   | enfuel   | low     | low        | NA       | f        | high       |
         | oil     | anecdote | refinera | low     | low        | a4       | f4       | high       |
         - I think the target will always have high statistics
           - Or maybe just for low valence?
           - Yes, it depends on valence
           - Also valence is positive/negative, not high/low
       - Anecdote + statistics amended:
         | project | role     | business | valence  | similarity | analysis | features | statistics |
         |---------+----------+----------+----------+------------+----------+----------+------------|
         | oil     | target   | enfuel   | positive | high       | NA       | f        | low        |
         | oil     | anecdote | refinera | positive | high       | a1       | f1       | low        |
         | oil     | target   | enfuel   | negative | high       | NA       | f        | high       |
         | oil     | anecdote | refinera | negative | high       | a2       | f2       | high       |
         | oil     | target   | enfuel   | positive | low        | NA       | f        | low        |
         | oil     | anecdote | refinera | positive | low        | a3       | f3       | low        |
         | oil     | target   | enfuel   | negative | low        | NA       | f        | high       |
         | oil     | anecdote | refinera | negative | low        | a4       | f4       | high       |
       - Also, statistics isn't really relevant to the anecdote; only to target.
         - But again, it seems to be useful for filtering, even though they're duplicated.
       - Statistics only:
         | project | role     | business | valence | similarity | analysis | features | statistics |
         |---------+----------+----------+---------+------------+----------+----------+------------|
         | oil     | target   | enfuel   | NA      | NA         | NA       | f        | high       |
         | oil     | anecdote | NA       | NA      | NA         | NA       | NA       | high       |
     - So we make one of those for each domain.
     - Pair them up.
     - Then in each pair each one either acts as a target or a comparison each time.
       - I guess we do this through some filtering and latin unnesting
       - After you filter down to a condition, you get a target and anecdote row
         for each domain.
       - Each of those gets a value 1 or 2 for "target/comparison variation"
       - Or I guess just duplicate everything and given them a 1 and 2.
     - No just to figure out how to counterbalance each pair and their
       within-subjects condition
       - Surely just do the same thing as above.
       - Yeah, give each of the five conditions a vector of five for "project
         pair within-subject condition variation"
**** In action
     - For each project type
       - Anecdote condition
       - valence
         - role
         - business name
     - Maybe just go for it
     - Make a working example without counterbalancing
     - So we essentially have five displays
       - statistics only
       - anecdote condition high alignment negative valence
       - anecdote condition high alignment positive valence
       - anecdote condition low alignment negative valence
       - anecdote condition low alignment positive valence
**** After rewriting the old materials as a placeholder
     - I think something like alignment 8 would be good
     - The end result is a tibble with a column that has a tibble of the displays
     - And each row of the original tibble is a condition that we pass to
     jaysire to make a conditional timeline
     - So each of those sub-tibbles should have five rows (statistics + four
     anecdote displays)
     - Between subjects IV is really just anecdote condition
       - Plus whatever other variation IVs we end up having
     - So the plan now is to first just get the minimum required for let's say
     the combined condition, with three displays.
     - And then I guess the variations would come from that
     - For that you can also split it up into initially just getting the target
     sorted, and then working out the displays from there.
     - Because supposedly the values come from there.
     - The trouble now is doing everything in rows, but also making sure that
     anecdote and target have opposite values
     - Perhaps using ~pivot~?
       - Yes, pivot works
       - Still not super pretty, because you have suffixes, but works.
     - Currently have ~project_feature_variation~.
       - But hard to see how that works with similarity manipulation
       - So maybe we don't need that level of variation.
       - Maybe varying the projects themselves between the five displays is enough
       - That is, any differences between anecdote conditions can't be attributed to
         content
       - What about for alignment manipulation?
         - Well it can't be due to content, because they're different.
       - Does that mean that we now don't need the whole pivoted target/anecdote
         shebang?
       - Well I guess we still need it, but the target project doesn't need to
         change
     - Not working with making the anecdote variation change with NPV.
       - So maybe we rewrite so that target is made first, and then anecdote after,
     without pivoting, inside each row's nested column
**** Justification of counterbalancing conditions
     - Anecdote variation
       - Otherwise it's unclear if people picked a project because of its
         contents
       - It's really also a "target project" variation
     - Project variation
       - We need five pairs of projects, for each of the five displays
       - But I guess project variation is more about which pairs go with which
     anecdote condition
     - So we need five pairs for each anecdote condition, so that we can
       assign each of the five pairs to the five conditions and choose one of
       five latin square "variations".
       - So one project variation condition filters to the five displays
**** Making project variation work
     - Project variation gives a number to (at this point) low and high
       alignment rows and then a different number to the same rows but flipped.
     - So that in one case domain 1 is low and domain 2 is high, and the
       opposite is the case with the other variation condition level.
*** DONE Realistic multipliers
    CLOSED: [2020-12-15 Tue 10:46]
    - Get from the same method as alignment 8
    - But actually a bit different
    - There's a difference between alignment conditions
      - Depends on the cut off, though.
      - And also on valence, no?
    - Example
      - Target
        - Oil: 2200L per hour
      - Negative valence high alignment
        - 2000
        - Cutoff: 3000
      - Negative valence low alignment
        - 1400
        - Cutoff: 2100
    - So for negative high the cutoff is above both values
      - anecdote failed, so the target will fail
    - For negative low the cutoff is above anecdote, but below target
      - anecdote failed, but target not necessarily
    - For positive high, the cutoff should be below both of them
      - anecdote worked, so target will work
    - For positive low, the cutoff should be below anecdote, but above target
      - anecdote worked, but target not necessarily
    | valence  | similarity | multiplier_anecdote | multiplier_cutoff | target | anecdote | cutoff |
    |----------+------------+---------------------+-------------------+--------+----------+--------|
    | negative | low        |                 0.7 |               0.9 |   2200 |    1540. |  1980. |
    | negative | high       |                 1.4 |               1.6 |   2200 |    3080. |  3520. |
    | positive | low        |                 1.6 |               1.4 |   2200 |    3520. |  3080. |
    | positive | high       |                 0.9 |               0.7 |   2200 |    1980. |  1540. |
    #+TBLFM: $6=$5 * $3::$7=$5 * $4

    - Should negative high anecdote be higher or lower thatn the target?
      - Either way the cutoff is higher than both
      - Seems as though it should be below so that there's less reason
    - But also shouldn't there be two stages of multiplication, rather than two
    separate ones?
    - Making the cutoff and then anecdote from that
    - Or the other way around
    | valence  | similarity | multiplier_cutoff | multiplier_anecdote | target | cutoff | anecdote |
    |----------+------------+-------------------+---------------------+--------+--------+----------|
    | negative | low        |               0.5 |                 0.5 |   2200 |  1100. |     550. |
    | negative | high       |               1.5 |                 0.5 |   2200 |  3300. |    1650. |
    | positive | low        |               1.5 |                 1.5 |   2200 |  3300. |    4950. |
    | positive | high       |               0.5 |                 1.5 |   2200 |  1100. |    1650. |
    #+TBLFM: $6=$5 * $3::$7=$6 * $4
    - That doesn't work, because it doesn't guarantee the anecdote - target
    relationship in the high similarity conditions
    - Regardless, what we need is a list of five (for each within-subjects combination)
      - Each element has a list of two (for the anecdote/target
        - Each of those has five combinations (for each domain)
          - etc
    - Double check this
*** DONE Analysis content
    CLOSED: [2020-12-24 Thu 15:30]
    - State "DONE"       from "TODO"       [2020-12-24 Thu 15:30]
    - Varies by
      - Project variation: different content per variation
      - Alignment: the same values as for the predicted features + cutoff
      - Valence: as above
    - Reason
      - Has to be different for each domain, but can have three components.
    - So the worry is that by having the analysis description be too specific,
    the target would appear more similar to such an extent that it is actually
    rational to go with the anecdote, because they both seem to share a causal
    structure. 
    - Analogy-wise this might be the same as the surface/higher-order distinction
    - so I guess ideally what we do is work out a way to describe the projects
      so that it's rational to always go for statistics.
      - However, as a back up we can just always end up descriptive and just point
      to the fact that people are not so dumb like in the analogy literature to simply
      use any indicator as relevance.

*** TODO Fix analysis content specifics
    - State "TODO"       from              [2020-12-24 Thu 15:30]
    - Including wording and multipliers
*** DONE Project variation content
    CLOSED: [2020-12-12 Sat 16:59]
*** TODO Clean up functions
*** DONE Input IDs
    CLOSED: [2020-12-11 Fri 15:51]
*** DONE Valence condition
    CLOSED: [2020-12-11 Fri 11:16]
    - Need to make sure valence positive targets have low statistics
    - And obviously that the analysis explains that the case study did well.
    - Unclear what to do with statistics only
      - Whether to have one per valence condition
      - Actually, you'd be using the other project's allocation
      - Or just stick to one condition and accept that the difference between
    valence conditions will be non-symmetrical
*** DONE Anecdote condition analysis differences
    CLOSED: [2020-12-11 Fri 10:29]
*** DONE Add project variation across within-subjects variables
    CLOSED: [2020-11-30 Mon 15:43]
    - State "DONE"       from "TODO"       [2020-11-30 Mon 15:43]
*** DONE Anecdote conditions
    CLOSED: [2020-12-11 Fri 10:29]
    - State "TODO"       from              [2020-11-30 Mon 15:49]
    - Maybe done by javascript?
    - Won't be easy, because it involves the NPV row
*** DONE Statistics only condition
    CLOSED: [2020-12-10 Thu 16:27]
    - State "DONE"       from "TODO"       [2020-12-10 Thu 16:27]
    - State "TODO"       from              [2020-11-30 Mon 15:49]
*** DONE Summary of plan
    CLOSED: [2020-12-07 Mon 09:27]
    - State "DONE"       from "TODO"       [2020-12-07 Mon 09:27]
    - Before investing further into the coding
*** DONE Fix location generation
    CLOSED: [2020-12-11 Fri 14:59]
    - Needs to vary by alignment and project type
    - Should end up a list that has low and high alignment
      - In each has anecdote and target
        - In each has the five pairs
*** Analysis
    - How are we comparing between Stats only and the others again?
    - I guess compare the difference stats only and the anecdote condition
      - And compare for each between subs condition.
    - Could get a "corrected" difference score of stats only vs anecdote condition
      - Only works if within-subjects, I think.
*** DONE Fix alignment correspondence with string value, structure, and integration
    CLOSED: [2020-12-12 Sat 09:45]
*** DONE Fix predicted features
    CLOSED: [2020-12-12 Sat 10:44]
    - At the moment the multiplier is both for the anecdote and comparison
    - Should be a totally different set of values for comparison, and the
    multiplier just for the anecdote
    - And the anecdote multiplier should be different for each similarity condition
    - So the 1 is for the target, and the non-zero multipliers for the anecdote
      
*** TODO Add reference class similarity description
* Aggregation
** Analysis
   - From Evan:
   #+begin_quote
   Ah right! Well I haven’t done that sort of thing before but I guess I’d
   probably start by looking at the number of alternations per 10 choices (is it
   a 2AFC type task?) or the average length of the run of the same choice (the
   two should be related of course)?
   #+end_quote

   - From Alex:
   #+begin_quote
   What you are describing makes sense though. You have less information in a
   binary outcome than in a continuous or even a richer categorical outcome. So
   it’s not possible to get a meaningful value for the autocorrelation if you don’t
   have any information on how it varies, as in a sequence of all 1s or 0s. It’s a
   similar issue to when you have a perfect predictor for a binary outcome. There
   is no information in that predictor, because there is perfect separation, so you
   have to exclude it from your model.

   #+end_quote
* Alignment
** DONE Experiment 8
   CLOSED: [2020-12-07 Mon 10:12]
   - State "DONE"       from "TODO"       [2020-12-07 Mon 10:12]
*** Plan  
    - Let's try work backwards:
    - Eventually we need to make a call to `trial_survey_multi_choice`, which creates timeline variables using `set_parameters`.
    - So each iteration of the loop should have a list of two for the two displays
    - I guess we can do everything in tibbles and then in the end nest the two displays (for reliability amount) in the end
      - And convert the two rows to a list
    - So everything can be on big tibble with the following variable columns:
      - Alignment condition
      - Reliability type
      - Project variation
        - Low alignment: intrinsic features
        - High alignment: project type
    - Then supposedly, filtering that down (and passing the relevant column value to display_if for condition) will get you the two displays for each combination
      - but actually it wouldn't be filtering, it would be stepping down each row and pulling the 'parameters'
    - Actually configuring the two reliability amount displays isn't that easy
      - They need different NPVs
      - Also, they need five new project descriptions
    - New issue
      - You ran Aggregation Experiment 3b (high alignment top up) with the Experiment 4 link.
      - So not the end of the world, but now what you probably need to do is to hard code those three IDs out of Experiment 4 prolific ID generation.
*** DONE Counterbalancing
    CLOSED: [2020-10-08 Thu 16:40]
    - Project variation
      - But this is randomised
    - Project name
      - Is it really just project name that needs latin square?
    - Also order of the values in each table, I guess.
      - Or maybe it's enough to just change the order of values, and we don't care so much about the order of the names
    - Ok so project name was definitely not as easy as I thought.
      - Maybe because of the alignment differences
      - So we're now going to try do it after everything
    - That worked fine
      - But might lead to errors when getting input data through, so pay attention.
    - And column order
    - We're currently on 12MB, so likely will need to revisit this and add column shuffling on the JS end
*** DONE Inputs
    CLOSED: [2020-10-09 Fri 11:50]
*** DONE Fix project aesthetics [5/5]
    CLOSED: [2020-11-03 Tue 09:50]
    - [X] Heading names in the final table
    - [X] Multiplier values
      - [X] Actual values
      - [X] Rounding
        - Done automatically due to the integer conversion
    - [NA] Allocation and ranking labels
      - Might be too hard to bother
      - Removed
      - The others didn't suggest to add this
    - [X] Business names
    - [X] Table width
    - [X] Add project type underline
*** DONE Make sure projects are different between displays
    CLOSED: [2020-10-12 Mon 18:13]
    - Different NPVs
    - Different projects for low alignment?
    - I guess you can do like in aggregation and sample pairs from the set of different projects
    - But what we can do is have "display A" and "display B"
      - And only five projects can appear in A and the other five in B
      - And we just randomise the order in jspsych
      - But then we're associating certain projects with low or high reliability
    - Instead we'd have to sample pairs like we said before
    - I guess it's a low vs high alignment issue again
      - With low alignment we can just have one of two variations
        - That is, either display A or B for low or high reliability
      - With high alignment that's where we might need to sample
      - Or not!
    - What if we just have a "project_display_variation" type of variable
      - And then within the variation of the display set, for high alignment you have the standard project_variation condition
      - So then I guess you're "merely" multiplying the conditions by two.
      - Add it at the beginning when adding project content
      - Then in the end nest the columns such that you get a tibble with a high and low reliability_amount column, a project_display_variation column that is `c(1,2)`, and the table contents
        - Supposedly then you'd have cases in which each display variation is associated with each reliability amount
*** DONE Figure out why ~materials_directory~ doesn't get rebuilt when testing is outdated.
    CLOSED: [2020-10-29 Thu 11:09]
    - Due to an upstream target not using the correct static branching map argument.
    - Specifically, ~testing~ had ~testing_directory~ as a dependency, and ~testing_directory~ had ~map(experiment_number)~ instead of ~.data = !!parameters~.
*** DONE Catch trials [2/2]
    CLOSED: [2020-10-17 Sat 12:51]
    - Can think of three types
      - Instructions check
      - Attention check
      - "Honesty check" (from [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6753310/#__sec25title][here]])
    - For the instructions check we can ask them which NPV is better
    - For attention check can be a trial between the two displays
      - Actually can be one before each display
      - "You will now see the first project display. It is important that you pay attention. Click the following checkbox before continuing on to the next page: [ ]. Please read through and complete the task accordingly."
    - Maybe also include a captcha?
      - Currently not working
      - Update: unlikely to happen, because requires update of psych server code
    - [[https://blog.prolific.co/how-to-improve-your-data-quality/][This article]] has a bunch of suggestions
      - And [[https://blog.prolific.co/minimising-noise-and-maximising-your-data-quality-the-case-of-satisficing/][here]]
    - Can also add something saying that you will get payment regardless of performance etc.
    - Let's reject if they get the NPV question and mid study attention check wrong
    - [X] Instructions check
    - [X] Attention check
*** DONE Generate the tables in JS
    CLOSED: [2020-10-10 Sat 15:43]
    - Will help with column order counterbalancing
      - And display pairs
    - So we just create the vectors/columns/whatever in R
      - Then jspsych takes them, shuffles, and puts them into a table from a function.
      - So we make a function whose argument is something like a vector of the columns/rows
        - And then the function itself already has the project and row names
        - The shuffling occurs in the function
    - So here's what you do:
      - Send to jspsych 1. an array of length five project columns, with each column as a vector (of 6 rows) in the array, 2. a vector for the header, 3. a vector for the row name column. For each table. Already in the function call.
      - Function steps:
        1. Shuffle the five vectors (columns)
        2. Add row name vector to the end of the array
        3. Transpose so that the rows are now columns
        4. Add header name vector
        5. Convert the new array of 6x6 into an HTML table
    - Would this help with display pairs?
      - Probably not actually
      - But what we can do is have "display A" and "display B"
        - And only five projects can appear in A and the other five in B
        - And we just randomise the order in jspsych
        - But then we're associating certain projects with low or high reliability
      - Instead we'd have to sample pairs like we said before
*** DONE Add reliability amount condition to input IDs
    CLOSED: [2020-10-17 Sat 13:38]
*** DONE Reanalyse old data using new techniques [3/3]
    CLOSED: [2020-11-03 Tue 09:50]
    - [X] difference between highest and lowest
      - Doesn't show an effect for Experiment 3 allocation
    - [X] mixed effect
      - Doesn't seem to work
      - Well, doesn't work when you try to play around with random effects
      - Works when you specify as in lm
      - Actually seems to be the best way to do this.
    - [X] covariate
      - but really just another within subject variable
      - But also: do we do project or npv amount?
        - Surely NPV amount
      - ANOVA or regression?
        - Seems equivalent
        - aov_ez doesn't seem to work with drake because character(0) isn't being taken as an argument
        - So we've got either aov_car or lm
          - Somehow lm seems to make more sense, because npv_amount is more continuous than categorical maybe?
        - Well how hard is it to do both?
          - Probably not that hard, but let's just start with lm
        - But now it seems that they're showing different estimates
        - So yeah let's do both
        - Ask informatics hub?
    - five regression
      - Asked Bruce to clarify
      - cancelled
*** DONE Add explanation of allocation task
    CLOSED: [2020-10-26 Mon 10:18]
    - Either in instructions or as preamble
*** DONE Generate test data
    CLOSED: [2020-10-22 Thu 17:48]
    - For some reason it isn't showing up with webdriver
    - But it has something to do with the main code, because welcome page works by itself
    - Also you changed around the experiment files for aggregation 4 and the resources for it
    - mock data files have also been edited a bit
**** DONE Add ad hoc webdriver code to satisfy ranking and allocation requirements [4/5]
     CLOSED: [2020-10-22 Thu 17:47]
     - [ ] Maybe add table class
       - Can also call "table", but probably better to use class in case we use different tables later
     - [X] Add ranking class
       - So that you can pull them out easier using webdriver
     - [X] Add allocation class
     - [X] Add ranking webdriver code
     - [X] Add allocation webdriver code
**** DONE Screenshots
     CLOSED: [2020-10-23 Fri 15:39]
*** CANCELLED Add project number to input ID
    - State "CANCELLED"  from "TODO"       [2020-11-19 Thu 15:03]
*** CANCELLED Change the NPV generation code a bit so that there isn't a duplicate value between sets
    - State "CANCELLED"  from "TODO"       [2020-11-19 Thu 15:03]
*** DONE Power analysis
    CLOSED: [2020-11-19 Thu 11:39]
    - Now that we're using ~lmer~ for analyses, we need to use something like
      ~simr~
    - What we can do is get previous experiments and then change the effect
      sizes as required.
    - I guess one issue is that we don't have a perfect pilot.
    - But we do have simulated data now.
    - Also we're meant to do sub component analyses?
      - Actually we're meant to do both to compare the sample needed
**** From http://finzi.psych.upenn.edu/R/library/simr/doc/fromscratch.html
     #+begin_src R
       library(simr)

       x <- 1:10
       g <- letters[1:3]
       X <- expand.grid(x = x, g = g)

       b <- c(2, -0.1) # fixed intercept and slope
       V1 <- 0.5 # random intercept variance
       V2 <- matrix(c(0.5, 0.05, 0.05, 0.1), 2) # random intercept and slope variance-covariance matrix
       s <- 1 # residual standard deviation

       model1 <- makeLmer(y ~ x + (1|g), fixef=b, VarCorr=V1, sigma=s, data=X)
       powerSim(model1, nsim=20)

     #+end_src

     #+RESULTS:

     - So I guess we just have to do it from scratch?
**** Test with already simulated data
     #+begin_src R
       library(drake)
       library(tidyverse)
       library(simr)

       loadd(data_simulation_alignment_8)

       formula <-
         allocation ~ alignment * reliability_amount * reliability_type * npv_amount + (1 | id)

       model <-
         formula %>%
         lmer(
           data = data_simulation_alignment_8
         )

       model %>%
         doTest(fcompare(~ alignment + reliability_amount))


       lm1 <- lmer(y ~ x + (x|g), data=simdata)
       lm0 <- lmer(y ~ x + (1|g), data=simdata)
       anova(lm1, lm0)
       compare(. ~ x + (1|g))(lm1)
       rcompare(~ (1|g))(lm1)
     #+end_src

     #+RESULTS:
     : 0.214325793064315

**** More research
     - Some useful resources all related to @singmann2019:
       - https://stats.stackexchange.com/questions/130714/how-to-choose-random-and-fixed-effects-structure-in-linear-mixed-models
       - http://singmann.org/mixed-models-for-anova-designs-with-one-observation-per-unit-of-observation-and-cell-of-the-design/
       - https://cran.r-project.org/web/packages/afex/vignettes/afex_mixed_example.html
**** So let's give it ago
     - Just following the afex::mixed vignette.
       #+begin_src R
         library(tidyverse)
         library(afex)
         library(drake)

         loadd(data_clean_alignment_8)

         ## look normal as is
         data_clean_alignment_8 %>% 
           mutate(
             log_allocation = log(allocation)
           ) %>%
           pivot_longer(cols = c(allocation, log_allocation),
                        names_to = "allocation_type",
                        values_to = "allocation") %>%
           ggplot(aes(allocation)) +
           geom_histogram(bins = 100) +
           facet_wrap(vars(allocation_type), scales = "free_x")

         model1 <-
           data_clean_alignment_8 %>% 
           mixed(
             allocation ~ alignment * reliability_type * reliability_amount * npv_amount + (alignment * reliability_type * reliability_amount * npv_amount | id),
             data = .
           )

         ## Warning messages:
         ## 1: Model failed to converge with 28 negative eigenvalues: -2.0e-01 -3.1e-01 -9.3e-01 -2.2e+00 -4.2e+00 -7.7e+00 -1.2e+01 -8.0e+01 -4.9e+02 -8.3e+02 -1.1e+03 -2.3e+03 -2.9e+03 -3.4e+03 -4.3e+03 -8.6e+03 -1.2e+04 -1.9e+04 -2.5e+04 -3.0e+04 -3.3e+04 -4.3e+04 -4.7e+04 -6.6e+04 -4.2e+05 -1.3e+06 -2.1e+06 -3.4e+06
         ## 2: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 3: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 4: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 5: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 6: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 7: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 8: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 9: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 10: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 11: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 12: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 13: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 14: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 15: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
         ## 16: Unable to compute Kenward-Roger F-test: using Satterthwaite instead

         ## Warning message:
         ##           lme4 reported (at least) the following warnings for 'full':
         ##                                                                 * boundary (singular) fit: see ?isSingular 

         summary(model1)$varcor
       #+end_src
**** Action plan
     - Seems like we're going to have to do the standard simulate, analyse, and iterate.
     - But how will we specify the effect sizes?
     - At the moment we're simulating allocation using the correlation.
     - You know what else we can do
       - Just determine a mean for each NPV amount!
     - Unless there's a standard way to simulate these kinds of continuous
       variables
     - Usually you just use ~rnorm~ or one of the other distributions
     - Technically we should be working out what kind of distribution this is
     - We can also generate y by running the regression backwards, as [[https://stats.stackexchange.com/questions/115748/simulate-data-for-2-x-2-anova-with-interaction/115767][this
       suggests]].
       - I think we can just use ~lme4::simulate.merMod()~
     - [[https://aosmith.rbind.io/2018/04/23/simulate-simulate-part-2/][This one]] suggests that we can just add npv amount in as is and you'll get
       the response variable as long as you specify the others.
     - But [[https://github.com/RInterested/SIMULATIONS_and_PROOFS/blob/master/Trees%20mixed%20random%20effects][this]] suggests more than one fixed factor might be more involved.
***** DeBruine and Barr (2019)
      - I think [[https://debruine.github.io/lmem_sim/articles/paper.html][this]] is a good guide, because it's recent and peer reviewed
      - Or we use her [[https://github.com/debruine/faux][faux]] package
      - DeBruine and Barr (2019) suggest that SR approximation is actually better
        for lmer's REML, and cite Luke (2017).
      - They also have an interesting mention of what to do without pilot data:
      #+begin_quote
      If you lack any pilot data to work with, you can start with the general
      rule of thumb setting the residual variance to about twice the size of the
      by-subject or by-item variance components (see supplementary materials
      from Barr et al., 2013 at
      https://talklab.psy.gla.ac.uk/simgen/realdata.html for results from an
      informal convenience sample).
      #+end_quote
      - Barr et al. (2013) also has some practical tips.
*****  [[https://debruine.github.io/tutorials/sim-lmer.html][DeBruine's sim-lmer tutorial]]
      - Also useful
      - Goes into interactions
        - But of categorical variables
        - And suggests to set the values in relation to the grand mean
        - Transforms them into main effects and interactions
      - Slopes
        - Suggests to only add within-subjects factors
        - Contrary to [[https://stats.stackexchange.com/a/408983][this example]].
        - But I guess we'll go with DeBruine?
        - Also just categorical
        - [[https://stats.stackexchange.com/a/162735][This]] is an example of using time, which is isimilar to NPV amount
***** What does {faux} have to offer?
      - Ideally it automates a lot of this stuff
      - Otherwise we'll do as per the tutorial
      - Yeah looks like it's too specific to designs with item random effects.
      - Unless maybe we can use ~sim_design()~?
        - Doesn't seem possible/simple
**** Actual action plan
     - Bring in alignment experiments 2, 3, and 7.
       - 2 has information about NPV amount, reliability amount, and alignment
         for explicit reliability,
       - 3 has information about NPV amount, reliability amount, and alignment
         for implicit reliability,
         - Although these results are different to 7
         - Here there was a main effect of alignment
         - So maybe let's not include 3
       - 7 has information about reliability amount and reliability type for
         alignment condition separately.
     - Model
       #+begin_src R
         allocation ~
           alignment * reliability_type * reliability_amount * npv_amount +
           (npv_amount * reliability_amount | id)
       #+end_src
     - Take relevant effects
       - 2
         - alignment fixed for explicit reliability
         - reliability amount fixed for explicit reliability
         - NPV amount fixed for explicit reliability
         - Subject variability
         - Subject x alignment correlation
         - Subject x reliability amount correlation
       - 7
     - Ok, let's just go for it
**** Not as expected
     - Hard because
       - Doing the four-way interaction in ~makeLmer~ means making up effect
         estimates and also means that the simulation may be garbled because of
         mixing estimates from different experiments.
       - Simulated data doesn't follow the same constraints as the actual
         options
         - Not adding up to 100, some values negative.
     - Doing it like we did for the hypothesis plots wouldn't work because you
       can't specify effects.
       - It's built using correlations with the npv amount
       - But maybe that's fine?
       - Follows the constraints
       - I guess we can also take the correlations from the old data
       - But either way, we're still going to use ~lmer~ to analyse.
       - We could do it in a really convoluted way and run it heaps of times and
         only take the simulations that have the specs we want.
     - Another way is just to specify the raw means, no?
       - For each condition/cell.
       - And each of those has a mean and sd.
       - But I guess then we can't run sensitivity analyses.
     - Hybrid?
       - Maybe we run it through ~makeLmer~ and then run it through a custom truncation
         - Was quick for the correlation truncation we did
       - But I guess then it wouldn't end up with the same specs
       - 
**** An attempt at the full four-way
     #+begin_src R
       library(drake)
       library(faux)
       library(tidyverse)
       library(afex)
       library(papaja)

       npv_amount <-
         seq(from = 400, to = 800, length.out = 5) %>%
         map(~ seq(from = .x, length.out = 51)) %>%
         map(~ sample(., size = 2, replace = T)) %>%
         transpose() %>%
         map(unlist)

       reliability_amount <- c("low", "high")
       reliability_type <- c("implicit", "explicit")
       alignment <- c("low", "high")
       display_variation <- seq_len(2)

       n <- 100

       id <- seq_len(n)

       counterbalanced_npv <-
         expand_grid(
           npv_amount,
           reliability_amount
         ) %>%
         mutate(
           display_variation = c(
             display_variation,
             display_variation %>%
             rev()
           ) %>%
             as.factor()
         ) %>%
         unnest(npv_amount) %>%
         arrange(display_variation)

       between <- lst(
         alignment,
         reliability_type,
         display_variation
       )

       within <- lst(
         reliability_amount,
         )

       df <-
         sim_design(
           within,
           between,
           n = 1, plot = FALSE, long = TRUE
         ) %>%
         left_join(counterbalanced_npv,
                   by = c("display_variation", "reliability_amount")
                   ) %>%
         arrange(id) %>%
         as_tibble()


       loadd(data_clean_alignment_2)
       loadd(data_clean_alignment_7)

       set_sum_contrasts()

       random_intercept_variance <- 0

       model_alignment_2 <-
         data_clean_alignment_2 %>%
         filter(reliability_amount != "noNPV") %>%
         ## mutate(
         ##   across(reliability_amount, ~ .x %>%
         ##                                fct_relevel("noNPV", "low")
         ##          )
         ## ) %>%
         nest_by(id, allocation, alignment, reliability_amount, npv_amount) %>%
         mixed(
           allocation ~ alignment * reliability_amount * npv_amount +
             (1 | id),
           data = .
         ) %>%
         .[["full_model"]] %>%
         broom.mixed::tidy()

       model_alignment_7 <-
         data_clean_alignment_7 %>%
         filter(alignment == "low") %>%
         nest_by(id, allocation, reliability_amount, reliability_type, npv_cond) %>%
         mixed(
           allocation ~ reliability_amount * reliability_type * npv_cond +
             (1 | id),
           data = .
         ) %>%
         .[["full_model"]] %>%
         broom.mixed::tidy()

       combined_value <-
         list(
           "(Intercept)",
           "sd__Observation"
         ) %>%
         map(
           function(term_value) {
             list(
               model_alignment_2,
               model_alignment_7
             ) %>%
               map_dbl(
                 ~ .x %>%
                   filter(term == term_value) %>%
                   pull(estimate)
               ) %>%
               mean()
           }
         ) %>%
         set_names(
           "intercept",
           "residual_sd"
         )

       model_alignment_7 %>%
         pull(term)

       model_alignment_2 %>%
         pull(term)

       estimate_label_alignment_2 <-
         c(
           "npv_amount",
           "alignment1",
           "alignment1:npv_amount",
           "reliability_amount1",
           "reliability_amount1:npv_amount",
           "alignment1:reliability_amount1",
           "alignment1:reliability_amount1:npv_amount"
         )

       estimate_alignment_2 <-
         estimate_label_alignment_2 %>%
         map(
           ~ model_alignment_2 %>% 
             filter(term == .x) %>%
             pull(estimate)
         ) %>%
         set_names(
           "npv_amount",
           "alignment",
           "npv_amount_alignment",
           "reliability_amount",
           "reliability_amount_npv_amount",
           "alignment_reliability_amount",
           "alignment_reliability_amount_npv_amount"
         )

       estimate_alignment_7 <-
         c(
           "reliability_type1",
           "reliability_amount1:reliability_type1",
           "reliability_amount1:reliability_type1:npv_cond1",
           "reliability_type1:npv_cond1"
         ) %>%
         map(
           ~ model_alignment_7 %>% 
             filter(term == .x) %>%
             pull(estimate)
         ) %>%
         set_names(
           "reliability_type",
           "reliability_amount_reliability_type",
           "reliability_amount_reliability_type_npv_cond",
           "reliability_type_npv_cond"
         )

       fixed_effects <-
         c(
           combined_value$intercept,
           estimate_alignment_2$npv_amount,
           estimate_alignment_2$alignment,
           estimate_alignment_2$reliability_amount,
           estimate_alignment_7$reliability_type,
           estimate_alignment_2$npv_amount_alignment,
           estimate_alignment_2$reliability_amount_npv_amount,
           estimate_alignment_2$alignment_reliability_amount,
           estimate_alignment_7$reliability_type_npv_cond,
           ## placeholders
           1,
           estimate_alignment_7$reliability_amount_reliability_type,
           estimate_alignment_2$alignment_reliability_amount_npv_amount,
           1,
           estimate_alignment_7$reliability_amount_reliability_type_npv_cond,
           1,
           1
         )

       model1 <-
         simr::makeLmer(allocation ~
                          npv_amount * alignment * reliability_amount * reliability_type +
                          (1 | id),
                        fixef = fixed_effects,
                        VarCorr = random_intercept_variance,
                        sigma = combined_value$residual_sd,
                        data = df
                        )

       data_simulation  <-
         model1 %>%
         model.frame()

       data_simulation %>%
         ggplot(aes(y = allocation,
                    x = npv_amount,
                    linetype = reliability_amount,
                    fill = reliability_amount
                    )) +
         facet_grid(
           cols = vars(reliability_type),
           rows = vars(alignment),
           labeller = "label_both"
         ) +
         geom_point(shape = 21, colour = "black", alpha = 0.7) +
         geom_smooth(method = "lm", colour = "black") +
         ## scale_fill_grey(start = 0.2, end = 0.8) +
         theme_apa(base_size = 10)

     #+end_src

     #+RESULTS:

     - Works ok, but has crazy values
     
**** A simpler model
     - Let's see if we get more normal values if we just use npv amount
     #+begin_src R
       library(drake)
       library(faux)
       library(tidyverse)
       library(afex)
       library(papaja)

       npv_amount <-
         seq(from = 400, to = 800, length.out = 5) %>%
         map(~ seq(from = .x, length.out = 51)) %>%
         map(~ sample(., size = 2, replace = T)) %>%
         transpose() %>%
         map(unlist)

       reliability_amount <- c("low", "high")
       reliability_type <- c("implicit", "explicit")
       alignment <- c("low", "high")
       display_variation <- seq_len(2)

       n <- 100

       id <- seq_len(n)

       counterbalanced_npv <-
         expand_grid(
           npv_amount,
           reliability_amount
         ) %>%
         mutate(
           display_variation = c(
             display_variation,
             display_variation %>%
             rev()
           ) %>%
             as.factor()
         ) %>%
         unnest(npv_amount) %>%
         arrange(display_variation)

       between <- lst(
         alignment,
         reliability_type,
         display_variation
       )

       within <- lst(
         reliability_amount,
         )

       df <-
         sim_design(
           within,
           between,
           n = 1, plot = FALSE, long = TRUE
         ) %>%
         left_join(counterbalanced_npv,
                   by = c("display_variation", "reliability_amount")
                   ) %>%
         arrange(id) %>%
         as_tibble()


       set_sum_contrasts()

       loadd(data_clean_alignment_2)
       loadd(data_clean_alignment_7)

       data_simple <-
         data_clean_alignment_2 %>%
         nest_by(id, allocation, alignment, reliability_amount, npv_amount) %>%
         filter(alignment == "high", reliability_amount == "high")


       data_simple %>%
         ggplot(aes(y = allocation,
                    x = npv_amount,
                    linetype = reliability_amount,
                    fill = reliability_amount
                    )) +
         ## facet_grid(
         ##   ## cols = vars(reliability_type),
         ##   rows = vars(alignment),
         ##   labeller = "label_both"
         ## ) +
         geom_point(shape = 21, colour = "black", alpha = 0.7) +
         geom_smooth(method = "lm", colour = "black") +
         ## scale_fill_grey(start = 0.2, end = 0.8) +
         theme_apa(base_size = 10)

       model_simple <-
         data_simple %>%
         mixed(
           allocation ~  npv_amount +
             (1 | id),
           data = .
         ) %>%
         .[["full_model"]] %>%
         broom.mixed::tidy()

       estimate_simple <-
         c(
           "(Intercept)",
           "npv_amount",
           "sd__Observation"
         ) %>%
         map(
           ~ model_simple %>% 
             filter(term == .x) %>%
             pull(estimate)
         ) %>%
         set_names(
           "intercept",
           "npv_amount",
           "residual_sd"
         )

       fixed_effects_2 <-
         c(
           estimate_simple$intercept,
           estimate_simple$npv_amount
         )

       model2 <-
         simr::makeLmer(allocation ~
                          npv_amount +
                          (1 | id),
                        fixef = fixed_effects_2,
                        VarCorr = 0,
                        sigma = estimate_simple$residual_sd,
                        data = df
                        )

       data_simulation_2  <-
         model2 %>%
         model.frame()

       data_simulation_2 %>%
         ggplot(aes(y = allocation,
                    x = npv_amount,
                    ## linetype = reliability_amount,
                    ## fill = reliability_amount
                    )) +
         ## facet_grid(
         ##   cols = vars(reliability_type),
         ##   rows = vars(alignment),
         ##   labeller = "label_both"
         ## ) +
         geom_point(shape = 21, colour = "black", alpha = 0.7) +
         geom_smooth(method = "lm", colour = "black") +
         ## scale_fill_grey(start = 0.2, end = 0.8) +
         theme_apa(base_size = 10)

     #+end_src

     #+RESULTS:

     - Better values.
     - Still some negatives

**** What if we focus on specific effects
     - We can just do an analysis for the explicit high alignment interaction
     - And the explicit low alignment interaction
     - Downside seems to be that then what do we do about the implicit condition?
       - Well, either we end up working out how to do a four-way nicely
       - Or maybe we just do different relevant combinations
       - Also, this doesn't take into account the main effect - alignment
***** Explicit high alignment

      #+begin_src R
        library(drake)
        library(faux)
        library(tidyverse)
        library(simr)
        library(afex)

        npv_amount <-
          seq(from = 400, to = 800, length.out = 5) %>%
          map(~ seq(from = .x, length.out = 51)) %>%
          map(~ sample(., size = 2, replace = T)) %>%
          transpose() %>%
          map(unlist)

        reliability_amount <- c("low", "high")
        reliability_type <- c("implicit", "explicit")
        alignment <- c("low", "high")
        display_variation <- seq_len(2)

        counterbalanced_npv <-
          expand_grid(
            npv_amount,
            reliability_amount
          ) %>%
          mutate(
            display_variation = c(
              display_variation,
              display_variation %>%
              rev()
            ) %>%
              as.factor()
          ) %>%
          unnest(npv_amount) %>%
          arrange(display_variation)

        between <- lst(
          alignment,
          reliability_type,
          display_variation
        )

        within <- lst(
          reliability_amount,
          )

        n <- 24

        df <-
          sim_design(
            within,
            between,
            n = n/8, plot = FALSE, long = TRUE
          ) %>%
          left_join(counterbalanced_npv,
                    by = c("display_variation", "reliability_amount")
                    ) %>%
          arrange(id) %>%
          as_tibble()

        df %>%
          pull(id) %>%
          unique() %>%
          length()

        set_sum_contrasts()

        loadd(data_clean_alignment_2)

        data_filtered <-
          data_clean_alignment_2 %>%
          filter(alignment == "high", reliability_amount != "noNPV")

        data_filtered %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")

        model_simple <-
          data_filtered %>%
          mixed(
            allocation ~  npv_amount * reliability_amount +
              (1 | id),
            data = .
          ) %>%
          .[["full_model"]] %>%
          broom.mixed::tidy()

        model3 <- 
          df %>%
          makeLmer(
            allocation ~  npv_amount * reliability_amount +
              (1 | id),
            fixef = model_simple %>%
              filter(effect == "fixed") %>%
              pull(estimate),
            VarCorr = 0,
            sigma = model_simple %>%
              filter(group == "Residual") %>%
              pull(estimate),
            data = .
          )

        data_simulation_3  <-
          model3 %>%
          model.frame()

        simulate_data <- function(df) {
          model3 <- 
            df %>%
            makeLmer(
              allocation ~  npv_amount * reliability_amount +
                (1 | id),
              fixef = model_simple %>%
                filter(effect == "fixed") %>%
                pull(estimate),
              VarCorr = 0,
              sigma = model_simple %>%
                filter(group == "Residual") %>%
                pull(estimate),
              data = .
            )

          data_simulation_3  <-
            model3 %>%
            model.frame()

          data_simulation_3 %>%
            mixed(
              allocation ~  npv_amount * reliability_amount +
                (1 | id),
              data = .
            ) %>%
            .[["full_model"]] %>%
            broom.mixed::tidy()
        }

        data_simulation_3 %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")


        powerSim(model3,
                 test = simr::fixed("npv_amount:reliability_amount1"),
                 nsim=1000)

        powerSim(model3,
                 test = fcompare(~ npv_amount + reliability_amount, "lr"),
                 nsim=20)


        simulation_results <-
          seq_len(100) %>%
          map_df(~ simulate_data(df))

        simulation_results %>%
          filter(effect == "fixed") %>%
          group_by(term) %>%
          summarise(
            mean_estimate = mean(estimate),
            mean_se = mean(std.error),
            sum(p.value < 0.05) %>% 
            binom.confint(100, level = 0.95, method = "exact") %>% 
            select(mean, lower, upper) %>% 
            rename(power = mean),
            .groups = "drop"
          )
        ## # A tibble: 4 x 6
        ##   term                           mean_estimate mean_se power lower upper
        ##   <chr>                                  <dbl>   <dbl> <dbl> <dbl> <dbl>
        ## 1 (Intercept)                          29.8    4.39     1    0.964 1    
        ## 2 npv_amount                           -0.0199 0.00688  0.8  0.708 0.873
        ## 3 npv_amount:reliability_amount1        0.0251 0.00688  0.98 0.930 0.998
        ## 4 reliability_amount1                 -12.5    4.37     0.81 0.719 0.882

      #+end_src
      - Looks like simr is a little annoying
      - So we'll do a mix of everything
      - From [[https://debruine.github.io/lmem_sim/articles/appendix1a_example_code.html#calculate-power-1][Debruine]] and the way we added simr to Micah's power analysis
***** Explicit low alignment

      #+begin_src R
        library(drake)
        library(faux)
        library(tidyverse)
        library(simr)
        library(afex)

        npv_amount <-
          seq(from = 400, to = 800, length.out = 5) %>%
          map(~ seq(from = .x, length.out = 51)) %>%
          map(~ sample(., size = 2, replace = T)) %>%
          transpose() %>%
          map(unlist)

        reliability_amount <- c("low", "high")
        reliability_type <- c("implicit", "explicit")
        alignment <- c("low", "high")
        display_variation <- seq_len(2)

        counterbalanced_npv <-
          expand_grid(
            npv_amount,
            reliability_amount
          ) %>%
          mutate(
            display_variation = c(
              display_variation,
              display_variation %>%
              rev()
            ) %>%
              as.factor()
          ) %>%
          unnest(npv_amount) %>%
          arrange(display_variation)

        between <- lst(
          alignment,
          reliability_type,
          display_variation
        )

        within <- lst(
          reliability_amount,
          )

        n <- 32

        df <-
          sim_design(
            within,
            between,
            n = n/8, plot = FALSE, long = TRUE
          ) %>%
          left_join(counterbalanced_npv,
                    by = c("display_variation", "reliability_amount")
                    ) %>%
          arrange(id) %>%
          as_tibble()

        df %>%
          pull(id) %>%
          unique() %>%
          length()

        set_sum_contrasts()

        loadd(data_clean_alignment_2)

        data_filtered <-
          data_clean_alignment_2 %>%
          filter(alignment == "low", reliability_amount != "noNPV")

        data_filtered %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")

        model_simple <-
          data_filtered %>%
          mixed(
            allocation ~  npv_amount * reliability_amount +
              (1 | id),
            data = .
          ) %>%
          .[["full_model"]] %>%
          broom.mixed::tidy()

        model3 <- 
          df %>%
          makeLmer(
            allocation ~  npv_amount * reliability_amount +
              (1 | id),
            fixef = model_simple %>%
              filter(effect == "fixed") %>%
              pull(estimate),
            VarCorr = 0,
            sigma = model_simple %>%
              filter(group == "Residual") %>%
              pull(estimate),
            data = .
          )

        data_simulation_3  <-
          model3 %>%
          model.frame()

        simulate_data <- function(df) {
          model3 <- 
            df %>%
            makeLmer(
              allocation ~  npv_amount * reliability_amount +
                (1 | id),
              fixef = model_simple %>%
                filter(effect == "fixed") %>%
                pull(estimate),
              VarCorr = 0,
              sigma = model_simple %>%
                filter(group == "Residual") %>%
                pull(estimate),
              data = .
            )

          data_simulation_3  <-
            model3 %>%
            model.frame()

          data_simulation_3 %>%
            mixed(
              allocation ~  npv_amount * reliability_amount +
                (1 | id),
              data = .
            ) %>%
            .[["full_model"]] %>%
            broom.mixed::tidy()
        }

        data_simulation_3 %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")

        nsim <- 100
        simulation_results <-
          seq_len(nsim) %>%
          map_df(~ simulate_data(df))

        simulation_results %>%
          filter(effect == "fixed") %>%
          group_by(term) %>%
          summarise(
            mean_estimate = mean(estimate),
            mean_se = mean(std.error),
            sum(p.value < 0.05) %>% 
            binom.confint(nsim, level = 0.95, method = "exact") %>% 
            select(mean, lower, upper) %>% 
            rename(power = mean),
            .groups = "drop"
          )
        ##   term                           mean_estimate mean_se power  lower upper
        ##   <chr>                                  <dbl>   <dbl> <dbl>  <dbl> <dbl>
        ## 1 (Intercept)                        12.4      3.35     0.96 0.901  0.989
        ## 2 npv_amount                          0.0152   0.00523  0.83 0.742  0.898
        ## 3 npv_amount:reliability_amount1      0.000572 0.00523  0.06 0.0223 0.126
        ## 4 reliability_amount1                -0.454    3.34     0.08 0.0352 0.152

      #+end_src

      #+RESULTS:

***** Explicit - three-way

      #+begin_src R
        library(drake)
        library(faux)
        library(tidyverse)
        library(simr)
        library(afex)
        library(binom)

        npv_amount <-
          seq(from = 400, to = 800, length.out = 5) %>%
          map(~ seq(from = .x, length.out = 51)) %>%
          map(~ sample(., size = 2, replace = T)) %>%
          transpose() %>%
          map(unlist)

        reliability_amount <- c("low", "high")
        reliability_type <- c("implicit", "explicit")
        alignment <- c("low", "high")
        display_variation <- seq_len(2)

        counterbalanced_npv <-
          expand_grid(
            npv_amount,
            reliability_amount
          ) %>%
          mutate(
            display_variation = c(
              display_variation,
              display_variation %>%
              rev()
            ) %>%
              as.factor()
          ) %>%
          unnest(npv_amount) %>%
          arrange(display_variation)

        between <- lst(
          alignment,
          reliability_type,
          display_variation
        )

        within <- lst(
          reliability_amount,
          )

        n <- 80

        df <-
          sim_design(
            within,
            between,
            n = n/8, plot = FALSE, long = TRUE
          ) %>%
          left_join(counterbalanced_npv,
                    by = c("display_variation", "reliability_amount")
                    ) %>%
          arrange(id) %>%
          as_tibble()

        df %>%
          pull(id) %>%
          unique() %>%
          length()

        set_sum_contrasts()

        loadd(data_clean_alignment_2)

        data_filtered <-
          data_clean_alignment_2 %>%
          filter(reliability_amount != "noNPV")

        data_filtered %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          facet_grid(
            ## cols = vars(reliability_type),
            rows = vars(alignment),
            labeller = "label_both"
          ) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")

        model_simple <-
          data_filtered %>%
          mixed(
            allocation ~  npv_amount * reliability_amount * alignment +
              (1 | id),
            data = .
          ) %>%
          .[["full_model"]] %>%
          broom.mixed::tidy()

        model3 <- 
          df %>%
          makeLmer(
            allocation ~  npv_amount * reliability_amount * alignment +
              (1 | id),
            fixef = model_simple %>%
              filter(effect == "fixed") %>%
              pull(estimate),
            VarCorr = 0,
            sigma = model_simple %>%
              filter(group == "Residual") %>%
              pull(estimate),
            data = .
          )

        data_simulation_3  <-
          model3 %>%
          model.frame()

        simulate_data <- function(df) {
          model3 <- 
            df %>%
            makeLmer(
              allocation ~  npv_amount * reliability_amount * alignment +
                (1 | id),
              fixef = model_simple %>%
                filter(effect == "fixed") %>%
                pull(estimate),
              VarCorr = 0,
              sigma = model_simple %>%
                filter(group == "Residual") %>%
                pull(estimate),
              data = .
            )

          data_simulation_3  <-
            model3 %>%
            model.frame()

          data_simulation_3 %>%
            mixed(
              allocation ~  npv_amount * reliability_amount * alignment +
                (1 | id),
              data = .
            ) %>%
            .[["full_model"]] %>%
            broom.mixed::tidy()
        }

        data_simulation_3 %>%
          ggplot(aes(y = allocation,
                     x = npv_amount,
                     linetype = reliability_amount,
                     fill = reliability_amount
                     )) +
          geom_point(shape = 21, colour = "black", alpha = 0.7) +
          geom_smooth(method = "lm", colour = "black")

        nsim <- 1000
        simulation_results <-
          seq_len(nsim) %>%
          map_df(~ simulate_data(df))

        simulation_results %>%
          filter(effect == "fixed") %>%
          group_by(term) %>%
          summarise(
            mean_estimate = mean(estimate),
            mean_se = mean(std.error),
            sum(p.value < 0.05) %>% 
            binom.confint(nsim, level = 0.95, method = "exact") %>% 
            select(mean, lower, upper) %>% 
            rename(power = mean),
            .groups = "drop"
          )

                                                # with 100 reps

        ## term                                   mean_estimate mean_se power lower upper
        ## <chr>                                          <dbl>   <dbl> <dbl> <dbl> <dbl>
        ##                                                                             1 (Intercept)                                 21.6     2.30     1    0.964 1    
        ## 2 alignment1                                   9.36    2.30     0.98 0.930 0.998
        ## 3 npv_amount                                  -0.00327 0.00359  0.21 0.135 0.303
        ## 4 npv_amount:alignment1                       -0.0184  0.00359  1    0.964 1    
        ## 5 npv_amount:reliability_amount1               0.0121  0.00359  0.93 0.861 0.971
        ## 6 npv_amount:reliability_amount1:alignm…       0.0127  0.00359  0.95 0.887 0.984
        ## 7 reliability_amount1                         -5.99    2.29     0.69 0.590 0.779
        ## 8 reliability_amount1:alignment1              -6.46    2.29     0.86 0.776 0.921

                                                # with 1000 reps

        ## term                                   mean_estimate mean_se power lower upper
        ## <chr>                                          <dbl>   <dbl> <dbl> <dbl> <dbl>
        ##                                                                             1 (Intercept)                                 21.5     2.21    1     0.996 1    
        ## 2 alignment1                                   8.86    2.21    0.987 0.978 0.993
        ## 3 npv_amount                                  -0.00302 0.00344 0.129 0.109 0.151
        ## 4 npv_amount:alignment1                       -0.0177  0.00344 0.999 0.994 1.00 
        ## 5 npv_amount:reliability_amount1               0.0125  0.00344 0.952 0.937 0.964
        ## 6 npv_amount:reliability_amount1:alignm…       0.0126  0.00344 0.959 0.945 0.970
        ## 7 reliability_amount1                         -6.28    2.20    0.828 0.803 0.851
        ## 8 reliability_amount1:alignment1              -6.34    2.20    0.817 0.792 0.841
      #+end_src
**** Back to four-way?
     - Three-way ended up going well
     - 80 participants for power of .95
     - A bit too good to be true
     - But also, that's for different materials
     - So we should do a sensitivity analysis for different effect sizes
     - And also try adding reliability type?
**** Interaction power
     - [[https://approachingblog.wordpress.com/2018/01/24/powering-your-interaction-2/][This]] explains why moderation needs a lot of participants
     - Based on the seemingly [[http://datacolada.org/17][classic Uri Simonsohn blog]].
     - Seems that the reason we got a relatively low N for the interaction is
     that the two-way is a cross over, so doesn't require more than the N of the
     simple effect
     - And I guess the three-way was around 2x the previous N
     - Posts like [[https://stats.stackexchange.com/a/35994][this]] suggest that really as long as you're simulating you're fine
**** What are the issues?
     - Non-realistic simulation values
       - Negative values
       - Doesn't necessarily add up to 100
       - I guess the way to fix this is to simulate manually, rather than with ~makeLmer~
     - Unclear what should be the four-way interaction estimate
       - And all the lower level estimates
     - Even if we focus on simple effects, we're still going to check the
     ombinus analysis
**** Manual simulation
     #+begin_src R
       library(tidyverse) # for data wrangling, pipes, and good dataviz
       library(lmerTest)  # for mixed effect models
       library(GGally)    # makes it high to plot relationships between variables
                                               # devtools::install_github("debruine/faux")
       library(faux)      # for simulating correlated variables

       options("scipen"=10, "digits"=4) # control scientific notation
       set.seed(8675309) # Jenny, I've got your number

       sub_n  <- 200 # number of subjects in this simulation
       sub_sd <- 0 # SD for the subjects' random intercept

       sub <- tibble(
         sub_id = 1:sub_n,
         sub_i  = rnorm(sub_n, 0, sub_sd), # random intercept
         alignment = rep(c("low","high"), each = sub_n/2) # between-subjects factor
       )

       trials <- crossing(
         sub_id = sub$sub_id, # get subject IDs from the sub data table
         reliability_amount = c("low", "high") # all subjects see both congruent and incongruent versions of all stimuli
       ) %>%
         left_join(sub, by = "sub_id") # includes the intercept and conditin for each subject

                                               # set variables to use in calculations below
       grand_i          <- 21.5 # overall mean DV
       alignment_eff     <- 8.86  # mean difference between conditions: low - high
       reliability_amount_eff <- -6.28  # mean difference between versions: incongruent - congruent
       alignment_reliability_amount_ixn <-  0  # interaction between version and condition
       error_sd         <- 18  # residual (error) SD


       dat <- trials %>%
         mutate(
                                               # effect-code subject condition and stimulus version
           alignment.e = recode(alignment, "high" = -0.5, "low" = +0.5),
           reliability_amount.e = recode(reliability_amount, "low" = -0.5, "high" = +0.5),
                                               # calculate error term (normally distributed residual with SD set above)
           err = rnorm(nrow(.), 0, error_sd),
                                               # calculate DV from intercepts, effects, and error
           dv = grand_i + sub_i + err +
             (alignment.e * alignment_eff) + 
             (reliability_amount.e * reliability_amount_eff) + 
             (alignment.e * reliability_amount.e * alignment_reliability_amount_ixn) # in this example, this is always 0 and could be omitted
         )


       ggplot(dat, aes(alignment, dv, color = reliability_amount)) +
         geom_hline(yintercept = grand_i) +
         geom_violin(alpha = 0.5) +
         geom_boxplot(width = 0.2, position = position_dodge(width = 0.9))

                                               # With an interaction

                                               # set variables to use in calculations below
       grand_i    <- 21.5
       alignment_low_reliability_high <- -(8.86/2)
       alignment_low_reliability_low <- +(8.86/2)
       alignment_high_reliability_high <- -(-6.28/2)
       alignment_high_reliability_low <- +(-6.28/2)
       error_sd   <-  1

                                               # calculate main effects and interactions from simple effects above

                                               # mean difference between easy and hard conditions
       alignment_eff     <- (alignment_high_reliability_high + alignment_high_reliability_low)/2 -
         (alignment_low_reliability_high + alignment_low_reliability_low)/2
                                               # mean difference between incongruent and congruent versions
       reliability_amount_eff <- (alignment_low_reliability_low + alignment_high_reliability_low)/2 - 
         (alignment_low_reliability_high + alignment_high_reliability_high)/2  
                                               # interaction between version and condition
       alignment_reliability_amount_ixn <- (alignment_high_reliability_low - alignment_high_reliability_high) -
         (alignment_low_reliability_low - alignment_low_reliability_high) 

       dat <- trials %>%
         mutate(
                                               # effect-code subject condition and stimulus version
           alignment.e = recode(alignment, "high" = -0.5, "low" = +0.5),
           reliability_amount.e = recode(reliability_amount, "low" = -0.5, "high" = +0.5),
                                               # calculate error term (normally distributed residual with SD set above)
           err = rnorm(nrow(.), 0, error_sd),
                                               # calculate DV from intercepts, effects, and error
           dv = grand_i + sub_i + err +
             (alignment.e * alignment_eff) + 
             (reliability_amount.e * reliability_amount_eff) + 
             (alignment.e * reliability_amount.e * alignment_reliability_amount_ixn)
         )

       ggplot(dat, aes(alignment, dv, color = reliability_amount)) +
         geom_hline(yintercept = grand_i) +
         geom_violin(alpha = 0.5) +
         geom_boxplot(width = 0.2, position = position_dodge(width = 0.9))

                                               # Try with just effect sizes

       grand_i          <- 21.5 # overall mean DV
       alignment_eff     <- 8.86  # mean difference between conditions: low - high
       reliability_amount_eff <- -6.28  # mean difference between versions: incongruent - congruent
       alignment_reliability_amount_ixn <-  -6.34  # interaction between version and condition
       error_sd         <- 18  # residual (error) SD

       dat <- trials %>%
         mutate(
                                               # effect-code subject condition and stimulus version
           alignment.e = recode(alignment, "high" = -0.5, "low" = +0.5),
           reliability_amount.e = recode(reliability_amount, "low" = -0.5, "high" = +0.5),
                                               # calculate error term (normally distributed residual with SD set above)
           err = rnorm(nrow(.), 0, error_sd),
                                               # calculate DV from intercepts, effects, and error
           dv = grand_i + sub_i + err +
             (alignment.e * alignment_eff) + 
             (reliability_amount.e * reliability_amount_eff) + 
             (alignment.e * reliability_amount.e * alignment_reliability_amount_ixn)
         )

       ggplot(dat, aes(alignment, dv, color = reliability_amount)) +
         geom_hline(yintercept = grand_i) +
         geom_violin(alpha = 0.5) +
         geom_boxplot(width = 0.2, position = position_dodge(width = 0.9))

                                               # With an interaction - fixed raw values to add up to 0

                                               # set variables to use in calculations below
       grand_i    <- 21.5
       alignment_low_reliability_high <- -1
       alignment_low_reliability_low <- +1
       alignment_high_reliability_high <- -(-6.28/2)
       alignment_high_reliability_low <- +(-6.28/2)
       error_sd   <-  1

                                               # calculate main effects and interactions from simple effects above

                                               # mean difference between easy and hard conditions
       alignment_eff     <- (alignment_high_reliability_high + alignment_high_reliability_low)/2 -
         (alignment_low_reliability_high + alignment_low_reliability_low)/2
                                               # mean difference between incongruent and congruent versions
       reliability_amount_eff <- (alignment_low_reliability_low + alignment_high_reliability_low)/2 - 
         (alignment_low_reliability_high + alignment_high_reliability_high)/2  
                                               # interaction between version and condition
       alignment_reliability_amount_ixn <- (alignment_high_reliability_low - alignment_high_reliability_high) -
         (alignment_low_reliability_low - alignment_low_reliability_high) 

       dat <- trials %>%
         mutate(
                                               # effect-code subject condition and stimulus version
           alignment.e = recode(alignment, "high" = -0.5, "low" = +0.5),
           reliability_amount.e = recode(reliability_amount, "low" = -0.5, "high" = +0.5),
                                               # calculate error term (normally distributed residual with SD set above)
           err = rnorm(nrow(.), 0, error_sd),
                                               # calculate DV from intercepts, effects, and error
           dv = grand_i + sub_i + err +
             (alignment.e * alignment_eff) + 
             (reliability_amount.e * reliability_amount_eff) + 
             (alignment.e * reliability_amount.e * alignment_reliability_amount_ixn)
         )

       ggplot(dat, aes(alignment, dv, color = reliability_amount)) +
         geom_hline(yintercept = grand_i) +
         geom_violin(alpha = 0.5) +
         geom_boxplot(width = 0.2, position = position_dodge(width = 0.9))

                                               # run original - without stim
       sub_n  <- 200 # number of subjects in this simulation
       sub_sd <- 0 # SD for the subjects' random intercept

       sub <- tibble(
         sub_id = 1:sub_n,
         sub_i  = rnorm(sub_n, 0, sub_sd), # random intercept
         sub_cond = rep(c("easy","hard"), each = sub_n/2) # between-subjects factor
       )

       trials <- crossing(
         sub_id = sub$sub_id, # get subject IDs from the sub data table
         stim_version = c("congruent", "incongruent") # all subjects see both congruent and incongruent versions of all stimuli
       ) %>%
         left_join(sub, by = "sub_id") # includes the intercept and conditin for each subject

                                               # set variables to use in calculations below
       grand_i    <- 400
       hard_congr <- +25
       hard_incon <- -25
       easy_congr <- -50
       easy_incon <- +50
       error_sd   <-  1


                                               # calculate main effects and interactions from simple effects above

                                               # mean difference between easy and hard conditions
       sub_cond_eff     <- (easy_congr + easy_incon)/2 -
         (hard_congr + hard_incon)/2
                                               # mean difference between incongruent and congruent versions
       stim_version_eff <- (hard_incon + easy_incon)/2 - 
         (hard_congr + easy_congr)/2  
                                               # interaction between version and condition
       cond_version_ixn <- (easy_incon - easy_congr) -
         (hard_incon - hard_congr) 


       dat <- trials %>%
         mutate(
                                               # effect-code subject condition and stimulus version
           sub_cond.e = recode(sub_cond, "hard" = -0.5, "easy" = +0.5),
           stim_version.e = recode(stim_version, "congruent" = -0.5, "incongruent" = +0.5),
                                               # calculate error term (normally distributed residual with sd set above)
           err = rnorm(nrow(.), 0, error_sd),
                                               # calculate dv from intercepts, effects, and error
           dv = grand_i + sub_i + err +
             (sub_cond.e * sub_cond_eff) + 
             (stim_version.e * stim_version_eff) + 
             (sub_cond.e * stim_version.e * cond_version_ixn)
         )


       ggplot(dat, aes(sub_cond, dv, color = stim_version)) +
         geom_hline(yintercept = grand_i) +
         geom_violin(alpha = 0.5) +
         geom_boxplot(width = 0.2, position = position_dodge(width = 0.9))


       group_by(dat, sub_cond, stim_version) %>%
         summarise(m = mean(dv) - grand_i %>% round(1)) %>%
         ungroup() %>%
         spread(stim_version, m)
     #+end_src
     - Debruine's example has negative simulated values, so maybe that's fine
     - And also it's almost just as hard to work out the raw values
       - Not to mention that they don't quite seem to correspond
     - And if you end up just using the effect sizes, then probably just do it
       through ~simr::makeLmer~
     - The issue with that was that it's hard to just add the four-way estimate,
     because all the other estimates are inter-dependent
     - So the idea was to go from the raw values
     - But that also seems limited, because it's not clear with Debruine's code
     how to have different patterns other than different sized interactions that
     don't have main effects
**** Another four-way attempt
     - Let's go back to the way we did three-way, but try adding reliability
     type and play around with it until it looks realistic
     #+begin_src R
       library(drake)
       library(faux)
       library(tidyverse)
       library(afex)
       library(papaja)

       npv_amount <-
         seq(from = 400, to = 800, length.out = 5) %>%
         map(~ seq(from = .x, length.out = 51)) %>%
         map(~ sample(., size = 2, replace = T)) %>%
         transpose() %>%
         map(unlist)

       reliability_amount <- c("low", "high")
       reliability_type <- c("implicit", "explicit")
       alignment <- c("low", "high")
       display_variation <- seq_len(2)

       n <- 100

       id <- seq_len(n)

       counterbalanced_npv <-
         expand_grid(
           npv_amount,
           reliability_amount
         ) %>%
         mutate(
           display_variation = c(
             display_variation,
             display_variation %>%
             rev()
           ) %>%
             as.factor()
         ) %>%
         unnest(npv_amount) %>%
         arrange(display_variation)

       between <- lst(
         alignment,
         reliability_type,
         display_variation
       )

       within <- lst(
         reliability_amount,
         )

       df <-
         sim_design(
           within,
           between,
           n = 1, plot = FALSE, long = TRUE
         ) %>%
         left_join(counterbalanced_npv,
                   by = c("display_variation", "reliability_amount")
                   ) %>%
         arrange(id) %>%
         as_tibble()


       loadd(data_clean_alignment_2)
       loadd(data_clean_alignment_7)

       set_sum_contrasts()

       random_intercept_variance <- 0

       model_alignment_2 <-
         data_clean_alignment_2 %>%
         filter(reliability_amount != "noNPV") %>%
         ## mutate(
         ##   across(reliability_amount, ~ .x %>%
         ##                                fct_relevel("noNPV", "low")
         ##          )
         ## ) %>%
         nest_by(id, allocation, alignment, reliability_amount, npv_amount) %>%
         mixed(
           allocation ~ alignment * reliability_amount * npv_amount +
             (1 | id),
           data = .
         ) %>%
         .[["full_model"]] %>%
         broom.mixed::tidy()

       model_alignment_7 <-
         data_clean_alignment_7 %>%
         filter(alignment == "low") %>%
         nest_by(id, allocation, reliability_amount, reliability_type, npv_amount) %>%
         mixed(
           allocation ~ reliability_amount * reliability_type * npv_amount +
             (1 | id),
           data = .
         ) %>%
         .[["full_model"]] %>%
         broom.mixed::tidy()

       combined_value <-
         list(
           "(Intercept)",
           "sd__Observation"
         ) %>%
         map(
           function(term_value) {
             list(
               model_alignment_2,
               model_alignment_7
             ) %>%
               map_dbl(
                 ~ .x %>%
                   filter(term == term_value) %>%
                   pull(estimate)
               ) %>%
               mean()
           }
         ) %>%
         set_names(
           "intercept",
           "residual_sd"
         )

       model_alignment_7 %>%
         pull(term)

       model_alignment_2 %>%
         pull(term)

       estimate_label_alignment_2 <-
         c(
           "npv_amount",
           "alignment1",
           "alignment1:npv_amount",
           "reliability_amount1",
           "reliability_amount1:npv_amount",
           "alignment1:reliability_amount1",
           "alignment1:reliability_amount1:npv_amount"
         )

       estimate_alignment_2 <-
         estimate_label_alignment_2 %>%
         map(
           ~ model_alignment_2 %>% 
             filter(term == .x) %>%
             pull(estimate)
         ) %>%
         set_names(
           "npv_amount",
           "alignment",
           "npv_amount_alignment",
           "reliability_amount",
           "reliability_amount_npv_amount",
           "alignment_reliability_amount",
           "alignment_reliability_amount_npv_amount"
         )

       estimate_alignment_7 <-
         c(
           "reliability_type1",
           "reliability_amount1:reliability_type1",
           "reliability_amount1:reliability_type1:npv_amount",
           "reliability_type1:npv_amount"
         ) %>%
         map(
           ~ model_alignment_7 %>% 
             filter(term == .x) %>%
             pull(estimate)
         ) %>%
         set_names(
           "reliability_type",
           "reliability_amount_reliability_type",
           "reliability_amount_reliability_type_npv_amount",
           "reliability_type_npv_amount"
         )

       fixed_effects <-
         c(
           combined_value$intercept,
           estimate_alignment_2$npv_amount,
           estimate_alignment_2$alignment,
           estimate_alignment_2$reliability_amount,
           estimate_alignment_7$reliability_type,
           ## 0,
           estimate_alignment_2$npv_amount_alignment,
           estimate_alignment_2$reliability_amount_npv_amount,
           estimate_alignment_2$alignment_reliability_amount,
           estimate_alignment_7$reliability_type_npv_amount,
           ## 0,
           ## placeholders
           0,
           ## estimate_alignment_7$reliability_amount_reliability_type,
           0,
           estimate_alignment_2$alignment_reliability_amount_npv_amount,
           0,
           ## estimate_alignment_7$reliability_amount_reliability_type_npv_amount,
           1,
           0,
           0
         )
       model1 <-
         simr::makeLmer(allocation ~
                          npv_amount * alignment * reliability_amount * reliability_type +
                          (1 | id),
                        fixef = fixed_effects,
                        VarCorr = random_intercept_variance,
                        sigma = 0 ## combined_value$residual_sd
                       ,
                        data = df
                        )
       data_simulation  <-
         model1 %>%
         model.frame()
       data_simulation %>%
         ggplot(aes(y = allocation,
                    x = npv_amount,
                    linetype = reliability_amount,
                    fill = reliability_amount
                    )) +
         facet_grid(
           cols = vars(reliability_type),
           rows = vars(alignment),
           labeller = "label_both"
         ) +
         geom_point(shape = 21, colour = "black", alpha = 0.7) +
         geom_smooth(method = "lm", colour = "black") +
         ## scale_fill_grey(start = 0.2, end = 0.8) +
         theme_apa(base_size = 10)

     #+end_src
     - So it seems as if you don't get such crazy values when changing npv_cond
       to npv_amount.
       - Just needed smaller estimates
       - So technically you just need to feed it the right pattern
       - And reducing the residual variance makes it clearer
       - What about using some of the previously simulated data

         #+begin_src R
           library(drake)
           library(faux)
           library(tidyverse)
           library(afex)
           library(papaja)

           npv_amount <-
             seq(from = 400, to = 800, length.out = 5) %>%
             map(~ seq(from = .x, length.out = 51)) %>%
             map(~ sample(., size = 2, replace = T)) %>%
             transpose() %>%
             map(unlist)

           reliability_amount <- c("low", "high")
           reliability_type <- c("implicit", "explicit")
           alignment <- c("low", "high")
           display_variation <- seq_len(2)

           n <- 100

           id <- seq_len(n)

           counterbalanced_npv <-
             expand_grid(
               npv_amount,
               reliability_amount
             ) %>%
             mutate(
               display_variation = c(
                 display_variation,
                 display_variation %>%
                 rev()
               ) %>%
                 as.factor()
             ) %>%
             unnest(npv_amount) %>%
             arrange(display_variation)

           between <- lst(
             alignment,
             reliability_type,
             display_variation
           )

           within <- lst(
             reliability_amount,
             )

           df <-
             sim_design(
               within,
               between,
               n = 80, plot = FALSE, long = TRUE
             ) %>%
             left_join(counterbalanced_npv,
                       by = c("display_variation", "reliability_amount")
                       ) %>%
             arrange(id) %>%
             as_tibble()


           loadd(data_clean_alignment_2)
           loadd(data_clean_alignment_7)

           set_sum_contrasts()

           random_intercept_variance <- 0

           model_alignment_2 <-
             data_clean_alignment_2 %>%
             filter(reliability_amount != "noNPV") %>%
             ## mutate(
             ##   across(reliability_amount, ~ .x %>%
             ##                                fct_relevel("noNPV", "low")
             ##          )
             ## ) %>%
             nest_by(id, allocation, alignment, reliability_amount, npv_amount) %>%
             mixed(
               allocation ~ alignment * reliability_amount * npv_amount +
                 (1 | id),
               data = .
             ) %>%
             .[["full_model"]] %>%
             broom.mixed::tidy()

           model_alignment_7 <-
             data_clean_alignment_7 %>%
             filter(alignment == "low") %>%
             nest_by(id, allocation, reliability_amount, reliability_type, npv_amount) %>%
             mixed(
               allocation ~ reliability_amount * reliability_type * npv_amount +
                 (1 | id),
               data = .
             ) %>%
             .[["full_model"]] %>%
             broom.mixed::tidy()

           combined_value <-
             list(
               "(Intercept)",
               "sd__Observation"
             ) %>%
             map(
               function(term_value) {
                 list(
                   model_alignment_2,
                   model_alignment_7
                 ) %>%
                   map_dbl(
                     ~ .x %>%
                       filter(term == term_value) %>%
                       pull(estimate)
                   ) %>%
                   mean()
               }
             ) %>%
             set_names(
               "intercept",
               "residual_sd"
             )

           model_alignment_7 %>%
             pull(term)

           model_alignment_2 %>%
             pull(term)

           estimate_label_alignment_2 <-
             c(
               "npv_amount",
               "alignment1",
               "alignment1:npv_amount",
               "reliability_amount1",
               "reliability_amount1:npv_amount",
               "alignment1:reliability_amount1",
               "alignment1:reliability_amount1:npv_amount"
             )

           estimate_alignment_2 <-
             estimate_label_alignment_2 %>%
             map(
               ~ model_alignment_2 %>% 
                 filter(term == .x) %>%
                 pull(estimate)
             ) %>%
             set_names(
               "npv_amount",
               "alignment",
               "npv_amount_alignment",
               "reliability_amount",
               "reliability_amount_npv_amount",
               "alignment_reliability_amount",
               "alignment_reliability_amount_npv_amount"
             )

           estimate_alignment_7 <-
             c(
               "reliability_type1",
               "reliability_amount1:reliability_type1",
               "reliability_amount1:reliability_type1:npv_amount",
               "reliability_type1:npv_amount"
             ) %>%
             map(
               ~ model_alignment_7 %>% 
                 filter(term == .x) %>%
                 pull(estimate)
             ) %>%
             set_names(
               "reliability_type",
               "reliability_amount_reliability_type",
               "reliability_amount_reliability_type_npv_amount",
               "reliability_type_npv_amount"
             )

           ##    effect  group  term              estimate std.error statistic    df   p.value
           ##    <chr>   <chr>  <chr>                <dbl>     <dbl>     <dbl> <dbl>     <dbl>
           ##  1 fixed   NA     (Intercept)        1.51e+1   2.60         5.82  184.   2.54e-8
           ##  2 fixed   NA     alignment1         9.78e+0   2.60         3.77  184.   2.21e-4
           ##  3 fixed   NA     reliability_amou… -1.02e+1   2.60        -3.92  184.   1.23e-4
           ##  4 fixed   NA     npv_amount         7.79e-3   0.00403      1.93  184.   5.48e-2
           ##  5 fixed   NA     reliability_type1 -5.54e+0   2.60        -2.14  184.   3.40e-2
           ##  6 fixed   NA     alignment1:relia… -1.05e+1   2.60        -4.05  184.   7.50e-5
           ##  7 fixed   NA     alignment1:npv_a… -1.56e-2   0.00403     -3.87  184.   1.50e-4
           ##  8 fixed   NA     reliability_amou…  1.62e-2   0.00403      4.03  184.   8.19e-5
           ##  9 fixed   NA     alignment1:relia…  1.04e+1   2.60         4.01  184.   8.97e-5
           ## 10 fixed   NA     reliability_amou… -9.35e+0   2.60        -3.60  184.   4.03e-4
           ## 11 fixed   NA     npv_amount:relia…  8.84e-3   0.00403      2.19  184.   2.96e-2
           ## 12 fixed   NA     alignment1:relia…  1.68e-2   0.00403      4.16  184.   4.83e-5
           ## 13 fixed   NA     alignment1:relia… -1.01e+1   2.60        -3.89  184.   1.42e-4
           ## 14 fixed   NA     alignment1:npv_a… -1.66e-2   0.00403     -4.12  184.   5.83e-5
           ## 15 fixed   NA     reliability_amou…  1.49e-2   0.00403      3.70  184.   2.83e-4
           ## 16 fixed   NA     alignment1:relia…  1.61e-2   0.00403      3.99  184.   9.43e-5
           ## 17 ran_pa… id     sd__(Intercept)    0.       NA           NA      NA   NA      
           ## 18 ran_pa… Resid… sd__Observation    7.99e+0  NA           NA      NA   NA      

           ##    term                                                         estimate
           ##    <chr>                                                           <dbl>
           ##  1 (Intercept)                                                  15.1    
           ##  2 alignment1                                                    9.78   
           ##  3 reliability_amount1                                         -10.2    
           ##  4 npv_amount                                                    0.00779
           ##  5 reliability_type1                                            -5.54   
           ##  6 alignment1:reliability_amount1                              -10.5    
           ##  7 alignment1:npv_amount                                        -0.0156 
           ##  8 reliability_amount1:npv_amount                                0.0162 
           ##  9 alignment1:reliability_type1                                 10.4    
           ## 10 reliability_amount1:reliability_type1                        -9.35   
           ## 11 npv_amount:reliability_type1                                  0.00884
           ## 12 alignment1:reliability_amount1:npv_amount                     0.0168 
           ## 13 alignment1:reliability_amount1:reliability_type1            -10.1    
           ## 14 alignment1:npv_amount:reliability_type1                      -0.0166 
           ## 15 reliability_amount1:npv_amount:reliability_type1              0.0149 
           ## 16 alignment1:reliability_amount1:npv_amount:reliability_type1   0.0161 
           ## 17 sd__(Intercept)                                               0      
           ## 18 sd__Observation                                               7.99   

           fixed_effects <-
             c(
               combined_value$intercept,
               estimate_alignment_2$npv_amount,
               estimate_alignment_2$alignment,
               estimate_alignment_2$reliability_amount,
               estimate_alignment_7$reliability_type,
               ## 0,
               estimate_alignment_2$npv_amount_alignment,
               estimate_alignment_2$reliability_amount_npv_amount,
               estimate_alignment_2$alignment_reliability_amount,
               estimate_alignment_7$reliability_type_npv_amount,
               ## 0,
               ## alignment1:reliability_type1
               1,
               estimate_alignment_7$reliability_amount_reliability_type,
               ## 0,
               estimate_alignment_2$alignment_reliability_amount_npv_amount,
               ## npv_amount:alignment1:reliability_type1
               -0.01,
               estimate_alignment_7$reliability_amount_reliability_type_npv_amount,
               ## 0,
               ## alignment1:reliability_amount1:reliability_type1
               -1,
               0.01
             )
           model1 <-
             simr::makeLmer(allocation ~
                              npv_amount * alignment * reliability_amount * reliability_type +
                              (1 | id),
                            fixef = fixed_effects,
                            VarCorr = random_intercept_variance,
                            sigma = 0 ## combined_value$residual_sd
                           ,
                            data = df
                            )
           data_simulation  <-
             model1 %>%
             model.frame()
           data_simulation %>%
             ggplot(aes(y = allocation,
                        x = npv_amount,
                        linetype = reliability_amount,
                        fill = reliability_amount
                        )) +
             facet_grid(
               cols = vars(reliability_type),
               rows = vars(alignment),
               labeller = "label_both"
             ) +
             geom_point(shape = 21, colour = "black", alpha = 0.7) +
             geom_smooth(method = "lm", colour = "black") +
             ## scale_fill_grey(start = 0.2, end = 0.8) +
             theme_apa(base_size = 10)

           simulate_data <- function(df) {
             model1 <-
               simr::makeLmer(allocation ~
                                npv_amount * alignment * reliability_amount * reliability_type +
                                (1 | id),
                              fixef = fixed_effects,
                              VarCorr = random_intercept_variance,
                              sigma = combined_value$residual_sd
                             ,
                              data = df
                              )

             data_simulation  <-
               model1 %>%
               model.frame()

             data_simulation %>%
               lmerTest::lmer(
                           allocation ~  npv_amount * reliability_amount *
                             alignment * reliability_type +
                             (1 | id),
                           data = .
                         ) %>%
               broom.mixed::tidy()
           }


           nsim <- 100
           simulation_results <-
             seq_len(nsim) %>%
             map_df(~ simulate_data(df))

           simulation_results %>%
             filter(effect == "fixed") %>%
             group_by(term) %>%
             summarise(
               mean_estimate = mean(estimate),
               mean_se = mean(std.error),
               sum(p.value < 0.05) %>% 
               binom.confint(nsim, level = 0.95, method = "exact") %>% 
               select(mean, lower, upper) %>% 
               rename(power = mean),
               .groups = "drop"
             )
         #+end_src
**** Another idea
     - The problem now seems to be that even when we combine alignment 2,
       alignment 7, and the hypotheses data, it's very hard to get the right
       combination of effects right
     - So what if do modelling/simulation of the effects of interest, and then
       combine the datasets?
**** DONE Specific effects
     CLOSED: [2020-11-16 Mon 15:42]
     - The effects
       - Explicit reliability alignment low vs high
       - High alignment implicit vs explicit
       - Low alignment implicit vs explicit
       - Null effects in implicit reliability
     - How to get
       - emmeans doesn't seem like it'll help because not sure how it handles
         "covariates"
       - Actually, we can, using ~emtrends~
       - But we don't actually need that anyway, right?
       - We just want the three and two-way interactions
       - So why don't we just filter and use lmer as usual?
       - But we can also use the ~by~ argument of emmeans to split it up and
         look at the individual trends/ compare them
       - Yep this seems to be the way.
       - e.g.
         #+begin_src R
           library(emmeans)

           ## data_simulation_raw <-
           ##   get_data_simulation_raw(df, estimates, formula)

           ## model <-
           ##   data_simulation_raw %>%
           ##   mixed(
           ##     formula,
           ##     data = .,
           ##     method = "S"
           ## )
           ## model %>%
           ##   emtrends(pairwise ~ reliability_amount | alignment * reliability_type, var = "npv_amount")

           emm_options(lmer.df = "Satterthwaite") # faster setting, preferrable
           emm_options(lmerTest.limit = 5200)

           fiber.lm <- lm(strength ~ diameter*machine, data=fiber)

                                                   # Suppose we want trends relative to sqrt(diameter)...
           emtrends(fiber.lm, ~ machine | diameter, var = "sqrt(diameter)", 
                    at = list(diameter = c(20, 30)))

         #+end_src
       - What trend differences do we want?
       - Really only in explicit high alignment
       - The other specific effects are interactions
       - Actually no, we care about slope differences between explicit and
         implicit low alignment, but averaged over reliability amount
       - And I guess between high and low alignment in low reliability
       - Summary:
         - Interactions
           - Four-way
             - [X] NPV amount x reliability amount x reliability type x alignment
           - Three-way
             - [X] High alignment: NPV amount x reliability amount x reliability type
             - [X] Explicit reliability: NPV amount x reliability amount x alignment
           - Two-way
             - [X] Low alignment: NPV amount x reliability type
         - Significant specific slopes
           - [X] High alignment explicit reliability: low vs high reliability
           - [X] Low explicit reliability: low vs high alignment
           - [X] Low alignment: implicit vs explicit reliability (averaged over reliability amount)
         - Null slopes
           - [X] Implicit reliability: low vs high alignment (averaged over reliability amount)
           - [X] High alignment implicit reliability: low vs high reliability amount
           - [X] Low alignment implicit reliability: low vs high reliability amount
       - Unclear if to use ~joint_tests~ or ~contrast~ for the separate interactions
         - Unclear also if ~joint_tests~ takes into account the slopes
         - ~contrast~ might be the safe option
         - Actually, yes they are equivalent. Squaring the t statistic in
           ~contrast~ leads to ~joint_tests~ F ratio
         - I guess just use ~contrast~ because it's a little more specific and
           you don't need as much filtering after tidying
       - Do we use ~Superpower::emmeans_power~?
         - Seems nice; calculates power and effect size for emmeans
         - But I guess we should just get the power from the same simulations right?
       - ~TOSTER~?
         - Seems emmeans equivalence is more powerful
         - Which seems to be [[https://github.com/Lakens/TOSTER/issues/43][acknowledged by Lakens]].
       - How do we determine ~delta~ for equivalence?
         - Surely just go to alignment 7 and see what the difference is there?
         - Lakens et al. (2018) talks about this
         - But what is delta, even?
         - Is it Cohen's d?
         - From ~?summary.emmGrid~:
           #+begin_quote
           Two-sided (equivalence) H_0: |theta - theta_0| >= delta versus
           H_1: |theta - theta_0| < delta
           t = (|Q - theta_0| - delta)/SE
           The p value is the _lower_-tail probability.
           Note that t is the maximum of t_{nonsup} and -t_{noninf}.
           This is equivalent to choosing the less significant result in
           the two-one-sided-test (TOST) procedure.
           #+end_quote
         - So I guess it's in terms of the raw measure
         - Just do an even value about alignment 7 difference?
         - Would it be an equivalent measure?
         - Should be, because out of 100, right?
         - I guess it's out of 400
         - Well, actually it's a difference in slopes
         - Why doesn't it work to just set it as the estimate, or a little
           bigger than it?
           - Surely because the CIs have to be within the delta, right?
           - Yeah, it should be the estimate +- CIs
           - Doesn't work either.
         - Lakens wants it to be the largest value that isn't significant in the pilot
           - But that doesn't work here
           - Calculated using:
             #+begin_src R
               BSDA::tsum.test(mean.x = 0.0145,
                               s.x = 0.007388672*sqrt(52), n.x = 52, mean.y = NULL, s.y = NULL,
                               n.y = NULL, alternative = "two.sided", mu = 0, var.equal = TRUE,
                               conf.level = 0.95
                               )
             #+end_src
           - Where the mean and SE are taken from the emmeans result for
             implicit low vs high reliability
         - So instead what we'll just do is the value that leads to a
           significant equivalence for alignment 7
         - 0.022 seems to be the value, with a p value of 0.0406
         - Which we got by doing this:
         #+begin_src R
           drake::loadd(data_clean_alignment_7)
           library(emmeans)

           model_alignment_7 <-
             data_clean_alignment_7 %>%
             filter(alignment == "low") %>%
             nest_by(id, allocation, reliability_amount, reliability_type, npv_amount) %>%
             mixed(
               allocation ~ reliability_amount * reliability_type * npv_amount +
                 (1 | id),
               data = .
             ) 

           model_alignment_7 %>%
             emtrends(spec = c("reliability_type", "reliability_amount"), var = "npv_amount") %>%
             pairs(by = "reliability_type") %>% 
             summary(infer = TRUE, side = "equivalence", delta = 0.022)
         #+end_src
**** New issues
     - Power doesn't increase with sample size
     - Need to check what is happening to the data when we increase n
     - Data seems fine.
     - Let's try do a simple t test and see if we can get the anticipated curves
     - [[https://stats.stackexchange.com/a/460872][This answer on CV]] suggests that maybe it's an issue of the model itself
       so maybe if we just fit a simpler model we'll be fine
     - Yeah, even just a simple t test gives very weird results.
       - That is, a power curve that is not smooth
     - So it's probably the initial model estimation, that we get the data from
     - We're getting the simulated data from the ~makeLmer~ function that is
       taking in the estimates from the previous experiment
     - I guess we can try changing the random error from 0 to not fit singular
       and if that doesn't work, simulate with ~lm~
       - Which I think you can do just from ~simulate~
     - Also, at the moment what we're doing is making an artificial dataframe,
       make up a model from estimates from old experiments and predicted, then
       we get the estimates from that model, and again get a model, get the data
       from that, and then run an analysis on that data.
     - ~simulate~ also works on the ~lmer~ model
       - Probably because of lme4 method
       - Yep ~simulate.merMod~
       - Also can take either model, or formula with old data
     - Also, don't forget that technically you should be doing a sensitivity
       analysis, rather than relying on the previous estimates
     - But also, if you're doing the alignment 2 and 7 analysis using ~lm~, why
       simulate using ~lmer~?
     - Simulating with lm doesn't have the same functionality as makeLmer, so
       seem like you have to first predict the values and then add them into the dataframe
     - Cool, that's fine, but now how do we generate implicit condition?
       - Before we specified estimates
       - But surely now that it's lm we can just either do it by hand, or use
         one of the packages we're looked at in the past, e.g., faux or simglm
       - Nah, you have the same problem as before with generating the NPV amount variable
       - You just did it by hand
**** More issues
     - Turns out the reason you had irregular curve was because you estimated
     new alignment 8 model parameters for each sample size
     - So now you fixed that and started using lm  
     - Problem with lm is that it doesn't seem to account for the
     within-subjects npv condition properly like ~afex::aov_car~ does.
     - We're just assuming that that's the way to go
     - The problem with using afex is that the way it does within-subjects is to
     convert the lm call into ~mlm~, which is a multivariate lm
     - The problem with that is that simulate can't handle it
     - So do we just go back to lmer, now that we worked out the curve irregularity?
     - But then we might as well just use lm
     - Yep, let's stick to lm
**** Effect size specification
     - We want to be able to simulate for different effect sizes
       - For a sensitivity analysis
     - Ideally with standardised effect sizes
       - In this case, partial eta squared.
     - The ~effectsize~ package should help
       - But for some reason, changing model coefficients doesn't lead to change
     in the standardised estimates
     - So maybe we just stick to the unstandardised ones
       - Should be interpreting them in terms of the allocation, anyway, right?
     - I guess we can also just calculate and display pes later
     - The other issue is that every time we estimate the alignment 8 model, we
     get quite different values
     - Even between -0.02 and 0.009
     - Because of the simulation of explicit and implicit y.
     - So maybe we get the values without the errors
     - So then we get the "true" values
     - Nah, we need the variance.
     - But only eventually.
     - So maybe just use the no-variance model to get the estimates, and then
     add them later
     - Ok, that works. But now isn't it just going to tell me to use the same
     amount of people as before, because we are technically doing a post-hoc analysis?
     - Well maybe, but also not necessarily, because the four-way interaction is new
*** DONE Fix form validation
    CLOSED: [2020-12-01 Tue 10:27]
    - You screwed up when "fixing" the phantomjs issue
    - But now you fixed it with a ~hasDuplicates~ function.
    - But now the conditional alert is screwing you over with the test variable
    - So let's work it out. Here's what you want:
    | has duplicates | test  | result |
    |----------------+-------+--------|
    | false          | false | false  |
    | false          | true  | false  |
    | true           | false | true   |
    | true           | true  | false  |
    - So it should just be what it was before, no?
    #+begin_src javascript
      var rank_duplicates = true
      var test = true

      if (rank_duplicates && !test) {
          console.log("alert")
      } else {
          console.log("continue")
      }

    #+end_src
*** DONE Fix interstitial order
    CLOSED: [2020-12-01 Tue 12:15]
    - Seems to be changing with randomisation of display order
    - Maybe we do what we did with the aggregation experiment and add a counter
*** DONE Fix materials pdf
    CLOSED: [2020-12-01 Tue 10:45]
    - Seems to have been a result of the form validation issues
*** DONE Elaborate on uniform distribution instructions
    CLOSED: [2020-12-07 Mon 10:12]
    - State "DONE"       from "TODO"       [2020-12-07 Mon 10:12]
    - State "TODO"       from              [2020-12-07 Mon 09:32]
*** DONE Fix mismatched input IDs
    CLOSED: [2020-12-09 Wed 14:18]
    - Generate the IDs after the shuffling in ~mutate_business_name_latin~
    - Combine the existing intrinsic values with the shuffled NPVs and business names
* Emacs
** DONE Spotify
   CLOSED: [2020-10-31 Sat 15:03]
** TODO Email
** DONE combine ESS configs
   CLOSED: [2020-10-20 Tue 10:05]
** TODO Work out how to work with the additional ESS config
   - That is, do we keep it in the layer, or user-config, or private layer?
** DONE Elaborate on syntax highlighting question
   CLOSED: [2020-10-03 Sat 19:00]
** TODO Figure out listviewer
** TODO Figure out ess-r-view-data
** DONE Line wrap without breaking up words
   CLOSED: [2020-11-03 Tue 09:40]
   - visual-line-mode
** TODO Evernote
** DONE Automatically start R REPL in project root
   CLOSED: [2020-10-20 Tue 10:08]
   - Looks like it's a feature-not-bug situation
   - You want to be asked, because otherwise there isn't an easy way of determining
   - RStudio has their .Rproj files, but it seems ESS doesn't want to do that
** TODO Get graphics device in buffer
** TODO Format while typing
* Life
** Work
   - Salary expectations
     - [[https://www.thebalancecareers.com/interview-questions-about-your-salary-expectations-2061235][Interview Question: "What Are Your Salary Expectations"]]
     - [[https://www.indeed.com/career-advice/interviewing/interview-question-what-are-your-salary-expectations][From Indeed]]
   - Salary estimation
     - https://www.totaljobs.com/
     - https://www.themuse.com/
     - https://www.seek.com.au/
     - https://www.glassdoor.com/index.htm
     - https://www.salary.com/
     - https://www.payscale.com/
     - https://www.indeed.com
     - 
   - Specific positions
     - [[https://www.seek.com.au/job/51109627?type=standard#searchRequestToken=556a4535-15ca-4c18-bc23-7f290ba85245][Advisor - Behavioural insights]]
       - 90k AUD
   - McKinsey
     - https://www.mckinsey.com/careers/search-jobs
   - behavioural economists
     - https://www.linkedin.com/in/dr-danielle-kent-372772b4/
     - https://www.linkedin.com/in/mcordonnier/
     - https://www.linkedin.com/in/jasonallancollins/
     - CBA
       - https://www.linkedin.com/in/william-r-mailer-6b23621b/
       - https://www.linkedin.com/in/nravichandar/
       - https://www.linkedin.com/in/etinosa-agbonlahor-2b588928/
       - https://www.linkedin.com/in/edaniels2/
       - https://www.linkedin.com/in/patrick-hendy-97aaa0155/
       - https://www.linkedin.com/in/daniel-campbell-44572b60/
   - [[https://www.indeed.com/career/associate-consultant/salaries?from=top_sb][Associate Consultant US]]
     - $69,193 USD
     - $7,000 profit sharing
   - [[https://www.glassdoor.co.uk/Salary/Behavioural-Insights-Team-Advisor-Salaries-E958568_D_KO26,33.htm][Behavioural Insights Team Advisor UK]]
     - £37,122
   - [[https://www.glassdoor.com/job-listing/behavioral-economics-research-professional-humana-JV_IC1137724_KO0,42_KE43,49.htm?jl=3735222386&pos=101&ao=242900&s=131&guid=0000017677c38c8cbf8769bc40de374a&src=GD_JOB_AD&t=FJ&extid=3&exst=&ist=OL&ast=OL&vt=w&slr=false&cs=1_cdbfbdcf&cb=1608327073057&jobListingId=3735222386&ctt=1608327111496][Behavioral Economics Research Professional Louisville, KY]]
     - $31K - $65K USD
   - [[https://www.glassdoor.com/job-listing/research-fellow-university-of-new-south-wales-JV_IC2235932_KO0,15_KE16,45.htm?jl=1006756855755&pos=101&ao=1136043&s=343&guid=0000017677deb7369b7d070bcb68ddc1&src=GD_JOB_AD&t=SR-JOBS-HR&vt=w&uido=E4493CEA7664EE1D7E8D9897CA5CD2B1&cs=1_19bd3b55&cb=1608328854027&jobListingId=1006756855755&jrtk=1-1eprtte9tu3n7801-1eprtteb4u2nr800-09f399701f8de383&ictk=1eprs7m4fu5a5801&ctt=1608347891724][Research Fellow Sydney]]
     - $109,365 - $128,926 AUD plus 17% superannuation and leave loading
   - [[https://www.glassdoor.com/Salaries/us-associate-professor-salary-SRCH_IL.0,2_IN1_KO3,22.htm?clickSource=searchBtn][Associate Professor Salaries in United States]]
     - $81,534 USD
   - [[https://www.glassdoor.com/Salary/Commonwealth-Bank-of-Australia-Associate-Sydney-Salaries-EJI_IE7922.0,30_KO31,40_IL.41,47_IM962.htm?experienceLevel=ONE_TO_THREE][Commonwealth Bank of Australia Associate in Sydney]] 1-3 years
     - A$82,662
   - [[https://www.glassdoor.com/Salary/PwC-Associate-Sydney-Salaries-EJI_IE8450.0,3_KO4,13_IL.14,20_IM962.htm?experienceLevel=ONE_TO_THREE][PwC Associate Salaries in Sydney 1-3 years]]
     - A$60,000
   - [[https://www.glassdoor.com/Salary/McKinsey-and-Company-Associate-Sydney-Salaries-EJI_IE2893.0,20_KO21,30_IL.31,37_IM962.htm?experienceLevel=ONE_TO_THREE][McKinsey & Company Associate Salaries in Sydney 1-3 years]]
     - A$157,333
* ARC
** Prospect theory 
   - The two examples in the 1993 paper:
     - With neutral probability weights
     - PT and CE should be equivalent according to the paper
   #+begin_src R
     library(pt)

     choice_ids <- c(1, 1, 1)
     gamble_ids <- c(1, 2, 2)
     outcome_ids <- c(1, 1, 2)
     objective_consequences <- c(300, 0, 1000)
     probability_strings <- c("1", "1/2", "1/2")
     my_choices <- Choices(choice_ids=choice_ids,
                           gamble_ids=gamble_ids,
                           outcome_ids=outcome_ids,
                           objective_consequences=objective_consequences,
                           probability_strings=probability_strings)
     my_choices

     drawChoices(my_choices,
                 decision_square_x=0.2, decision_square_edge_length=0.05,
                 circle_radius=0.025, y_split_gap=0.15, x_split_offset=0.03,
                 probability_text_digits=4, y_probability_text_offset=0.015,
                 y_value_text_offset=0.005, x_value_text_offset=0.025,
                 probability_text_font_colour="red", probability_text_font_size=10,
                 objective_consequence_text_font_colour="blue",
                 objective_consequence_text_font_size=10, label=c("A","B"),
                 label_font_colour=c("orange","magenta"), label_font_size=c(11,11),
                 label_positions=list(c(0.26,0.7),c(0.26,0.36)))

     kl_1993_utility <- Utility(fun="power", par=c(alpha=0.575, beta=0.575, lambda=2.5))

     tk_1992_positive_probWeight <-
       ProbWeight(fun=
                    "Tversky_Kahneman_1992",
                  par=c(alpha=1))

     tk_1992_negative_probWeight <-
       ProbWeight(fun=
                    "Tversky_Kahneman_1992",
                  par=c(alpha=1))

     comparePT(my_choices,
               prob_weight_for_positive_outcomes=
                 tk_1992_positive_probWeight,
               prob_weight_for_negative_outcomes=
                 tk_1992_negative_probWeight,
               utility=kl_1993_utility,
               digits=4)
   #+end_src
   - PT and CE for the two gambles here are not equivalent
     - Actually should be more like a \lambda of 1.7
   #+begin_src R
     library(pt)

     choice_ids <- c(1, 1, 1)
     gamble_ids <- c(1, 2, 2)
     outcome_ids <- c(1, 1, 2)
     objective_consequences <- c(0, 250, -100)
     probability_strings <- c("1", "1/2", "1/2")
     my_choices <- Choices(choice_ids=choice_ids,
                           gamble_ids=gamble_ids,
                           outcome_ids=outcome_ids,
                           objective_consequences=objective_consequences,
                           probability_strings=probability_strings)
     my_choices

     drawChoices(my_choices,
                 decision_square_x=0.2, decision_square_edge_length=0.05,
                 circle_radius=0.025, y_split_gap=0.15, x_split_offset=0.03,
                 probability_text_digits=4, y_probability_text_offset=0.015,
                 y_value_text_offset=0.005, x_value_text_offset=0.025,
                 probability_text_font_colour="red", probability_text_font_size=10,
                 objective_consequence_text_font_colour="blue",
                 objective_consequence_text_font_size=10, label=c("A","B"),
                 label_font_colour=c("orange","magenta"), label_font_size=c(11,11),
                 label_positions=list(c(0.26,0.7),c(0.26,0.36)))

     kl_1993_utility <- Utility(fun="power", par=c(alpha=0.575, beta=0.575, lambda=2.5))
     tk_1992_positive_probWeight <-
       ProbWeight(fun=
                    "Tversky_Kahneman_1992",
                  par=c(alpha=1))
     tk_1992_negative_probWeight <-
       ProbWeight(fun=
                    "Tversky_Kahneman_1992",
                  par=c(alpha=1))

     comparePT(my_choices,
               prob_weight_for_positive_outcomes=
                 tk_1992_positive_probWeight,
               prob_weight_for_negative_outcomes=
                 tk_1992_negative_probWeight,
               utility=kl_1993_utility,
               digits=4)
   #+end_src
   - If exponents (\alpha and \beta) and loss aversion coefficient (\lambda) are all 1, then
     the function is equivalent to EV (linear)
   #+begin_src R
     library(pt)

     plotUtility(my_x_label = "objective consequence",
                 my_y_label = "subjective value",
                 xmin = -10, xmax = 10, fun=power_uf,
                 par=c(alpha = 1, beta = 1, lambda = 1),
                 fun_colour = "purple",
                 draw_reference_line_flag = TRUE,
                 reference_line_colour = "red",
                 reference_line_style = 1)
   #+end_src
   - Reducing \alpha and \beta reflects diminishing returns, as in EU.
   #+begin_src R
     library(pt)

     plotUtility(my_x_label = "objective consequence",
                 my_y_label = "subjective value",
                 xmin = -10, xmax = 10, fun=power_uf,
                 par=c(alpha = 0.575, beta = 0.575, lambda = 1),
                 fun_colour = "purple",
                 draw_reference_line_flag = TRUE,
                 reference_line_colour = "red",
                 reference_line_style = 1)
   #+end_src
   - Increasing \lambda reflects the steeper curve in the domain of losses, as in PT.
   #+begin_src R
     library(pt)

     plotUtility(my_x_label = "objective consequence",
                 my_y_label = "subjective value",
                 xmin = -10, xmax = 10, fun=power_uf,
                 par=c(alpha = 0.575, beta = 0.575, lambda = 2.5),
                 fun_colour = "purple",
                 draw_reference_line_flag = TRUE,
                 reference_line_colour = "red",
                 reference_line_style = 1)
   #+end_src
** 1993 paper
   - p. 21
   #+begin_quote
   By the individual's own utility function, the cost of considering these
   gambles in iso- lation is 27% of their expected value, surely more than any
   rational decision maker should be willing to pay for whatever mental economy
   this isolation achieves.
   #+end_quote
   - Expressing the effect of aggregation as being the comparison between PT
     values with aggregation and PT values in isolation
   - p. 22
     #+begin_quote
     Decisions will be narrowly framed even when they could be viewed as
     instances of a category of similar decisions
     #+end_quote
     - A way to relate to analogy?
     - Also here (p. 23):
#+begin_quote
The explicit adoption of a broad frame will then require the use of an abstract
language that highlights the important common dimensions of diverse decision
problems.
#+end_quote
- But I guess they're talking about money and probability
** Ideas
   - Could test the idea that executives are worried about themselves while
     realising that they aren't acting in the benefit of the company
   - Can manipulate whether the participant thinks that they're evaluating
   subordinates' options, or they are the subordinate.
   - Test in real data the idea (from p.23) that oil and pharma are more likely
     to bracket broadly.
   - The broader questions could be:
     - how do managers bracket repeated gambles without feedback
       - what's the line between the risk aversion of a single gamble and
         displaying the distribution? Is there a way to structure the decisions
         such that aggregation is more likely?
     - can aggregation be encouraged
       - What decision aids encourage aggregation? For instance, verbal or
         visual cues.
   - Can first get participant's loss aversion coefficient and then give them
     specific gambles that they should technically be fine with.
     - Everyone is personalised
     - Would just be hard to have that all happen on the user side
