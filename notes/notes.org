* General
** DONE Give each experiment its own resource folder
   CLOSED: [2020-10-12 Mon 09:51]
   - But maybe it's ok to use a communal one.
** DONE Split up the omnibus data files before data cleaning so that everything isn't rerun after every new import
   CLOSED: [2020-10-26 Mon 09:47]
** TODO Read "The allocation of resources within firms"
** Prolific
   - Don't recruit multiple versions of the same study at once
     - You can't set exclusions for studies that haven't been 'completed'
** TODO Open {drake} question about "skipping" static branching targets
** Mixed models
*** Subject variance = zero
    - Note, we get singularity even for minimal model for alignment 2
      - [[https://stats.stackexchange.com/a/112435][This]] suggests that you can keep as is, or even just do a normal
        regression in that case.
    - Turns out this is also the case for alignment 7
    - So do we just simulate the data with subject variance of zero?
    - [[https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022509.html][Here]] Bates suggests that variance of zero just suggests the model should
    be simplified 
     - Also:
    #+begin_quote
    It is much more likely that there is insufficient data to estimate the
parameters in a model of this level of complexity.
    #+end_quote
   - According to [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/][this]], random effects...
   #+begin_quote
    are quite ‘data hungry’; requiring at least five ‘levels’ (groups) for a
    random intercept term to achieve robust estimates of variance (Gelman &
    Hill, 2007; Harrison, 2015). With <5 levels, the mixed model may not be able
    to estimate the among-population variance accurately. In this case, the
    variance estimate will either collapse to zero, making the model equivalent
    to an ordinary GLM (Gelman & Hill, 2007, p. 275) or be non-zero but
    incorrect if the small number of groups that were sampled are not
    representative of true distribution of means (Harrison, 2015).
   #+end_quote
   - But [[https://stats.stackexchange.com/a/466286][here]] Ben Bolker says it's fine to keep it even if fit is singular.
* Anecdotes
** Conceptual discussion
   - Last reply from Micah:
     #+begin_quote
     Meant to reply. I agree, that in the case that knowledge is lacking,
overall similarity is a smart strategy. But, in the case where you have
low-knowledge/overall similarity info of an individual case, but also know that
individual case is an outlier from the overall trend from a larger sample, do
you think that overall similarity should still be weighed heavily? It's
specifically the low-knowledge strategy when in conflict with large samples that
seem problematic to me. But does formal analysis or simulations suggest it is
still a good strategy? Or is this highly dependent on the degree of variability
in the large sample?
     #+end_quote
** Experiment 2
*** Design
    - IVs
      - Anecdote
      - Alignment
      - Polarity
    - DV
      - allocation
    - But the tricky thing is how we planned the new within-subjects design.
      - Anecdotes article Efperiment 2 actually summarises it:
    #+begin_quote
Efperiment 2 was very similar to [Efperiment 1], efcept that we added a
within-subjects anecdote valence manipulation. Further, manipulated anecdote
similarity within-subjects, in order to increase the efperiment's power. All
participants saw the statistics only condition, as it did not contain an
anecdote, and therefore did not need to be manipulated between-subjects. As
such, each participant saw five displays, with one statistics only condition,
and four displays for either the anecdote only condition, or the statistics and
anecdote condition. These four displays consisted of the similarity (low and
high) $\times$ valence (negative and positive) conditions.
        #+end_quote
    - How many domains?
      - Each display has two, and then the anecdote is the same as one of them.
      - So we cycle between each of them as the anecdote/reference.
      - So we'd need an anecdote analysis for each.
      - Eight in total.
    - How do we structure this?
      - So the end result is a df with rows that indicate anecdote condition and
        whatever counterbalancing we need.
      - Each row has a nested df with five rows.
        - One for each within-subjects condition (statistics only, and anecdote
          condition f valence f similarity.
      - Each of those rows has a nested column with the actual HTML display
        - Which is either just a table for the statistics only, or a table and a
          paragraph for the anecdote condition.
        - But how do we do that? For alignment we sent off just the columns so
          that you can shuffle the columns.
        - But I guess here you only have two projects.
        - So maybe we can just counterbalance anyway without bloating the
          efperiment file too much.
      - But the main source of counterbalancing will be the domains
        - Don't we actually need 10 domains, because the statistics only display
          also is displaying two projects.
        - Either way, we need  away of counterbalancing which domains go to
          which displays.
        - And which projects go to anecdote vs comparison.
        - Actually maybe it's not too bad.
          - Two/three anecdotes conditions (depending on if we do enhanced or not)
          - Five displays (five pairs of projects)
          - Two anecdote/comparison states (which of the pair is the anecdote
            and which is the comparison project)
          - 2/3 f 5 f 2
          - So 20/30 rows of five displays (of two projects each)
          - And double that if we counterbalance column order
            - But seems that we're better off to make a function for this like
              in alignment.
              - Takes rows as arguments, but also an anecdote argument.
      - So each domain needs different versions
        - low vs high intrinsic values
          - to be the anecdote or comparison, depending on valence condition
        - That's it really
        - Another way of putting it is that each needs
          - Anecdote description/analysis
          - High intrinsic features
          - Low intrinsic features
      - So we make a data frame with 10 rows for each domain
        - Each has the three components each needs as columns
        - Or two rows each because you can classify the two intrinsic features
          on a different column
          - And also because you always need an anecdote and one of the
            intrinsic features.
        - Then I guess we pair them
        - No, we do like in the alignment efperiment
          - Create a latin nested column, and the unnest, so that each domain
            has one of the variation values.
        - But we should still pair them up.
          - So that you get five pairs.
          - And then you assign each pair a vector of 1-5 for their variation.
        - So that's for within-subjects allocation
          - Wait a second, each domain also needs two valence and two similarity condition.
        - So we'll have three main columns:
          - Anecdote (including description and analysis)
          - Intrinsic features (high and low)
          - Statistics (high, low, NA (but only for NA anecdote))
        - For anecdote, we'll have five rows:
          - High valence, high similarity
          - High valence, low similarity
          - Low valence, low similarity
          - Low valence, high similarity
          - NA
        - But the teft of the anecdote depends on the intrinsic features
          - So I guess we actually mean "the descriptive components of the anecdote".
      - Also remember that all of this is just for the "low similarity"
        condition, because in high similarity they're comparing two projects
        from the same domain.
        - But the above technically already accounts for this because to do this
          we just need high and low value conditions
         | project | business | valence | similarity | statistics amount | anecdote | intrinsic |
         |---------+----------+---------+------------+-------------------+----------+-----------|
         | oil     | fuel co  | high    | low        | high              |          |           |
         | oil     | fuel co  | low     | low        | high              |          |           |
         | oil     | fuel co  | high    | high       | high              |          |           |
         | oil     | fuel co  | low     | high       | high              |          |           |
         | oil     | fuel co  | NA      | NA         | NA                |          |           |
         | oil     | refinera | high    | low        | low               |          |           |
         | oil     | refinera | low     | low        | low               |          |           |
         | oil     | refinera | high    | high       | low               |          |           |
         | oil     | refinera | low     | high       | low               |          |           |
         | oil     | refinera | NA      | NA         | NA                |          |           |
**** After reviewing Experiment 1
     - High and low similarity don't mean different business names
       - They have different business names regardless of similarity condition
       - They mean qualitative features that are similar (e.g., location) and
         quantitative values that are relevant
     - So each domain gets five components:
       1. Target project
       2. Anecdote - low valence high similarity
       3. Anecdote - high valence high similarity
       4. Anecdote - low valence low similarity
       5. Anecdote - high valence low similarity
         | project | project role | business | valence | similarity | analysis | features |
         |---------+--------------+----------+---------+------------+----------+----------|
         | oil     | target       | enfuel   | high    | high       | NA       | f        |
         | oil     | anecdote     | refinera | high    | high       | a1       | f1       |
         | oil     | target       | enfuel   | low     | high       | NA       | f        |
         | oil     | anecdote     | refinera | low     | high       | a2       | f2       |
         | oil     | target       | enfuel   | high    | low        | NA       | f        |
         | oil     | anecdote     | refinera | high    | low        | a3       | f3       |
         | oil     | target       | enfuel   | low     | low        | NA       | f        |
         | oil     | anecdote     | refinera | low     | low        | a4       | f4       |
     - And then when we filter by condition we always get the same target, and
       one of the four anecdotes
     - But I guess we have stats only
     - So we add a statistics column (we'll sketch just with one similarity condition)
       - But actually similarity doesn't mean anything in statistics only
       - Nor does valence.
       - So let's just add on
       - Anecdote only:
         | project | role     | business | valence | similarity | analysis | features | statistics |
         |---------+----------+----------+---------+------------+----------+----------+------------|
         | oil     | target   | enfuel   | high    | high       | NA       | f        | NA         |
         | oil     | anecdote | refinera | high    | high       | a1       | f1       | NA         |
         | oil     | target   | enfuel   | low     | high       | NA       | f        | NA         |
         | oil     | anecdote | refinera | low     | high       | a2       | f2       | NA         |
         | oil     | target   | enfuel   | high    | low        | NA       | f        | NA         |
         | oil     | anecdote | refinera | high    | low        | a3       | f3       | NA         |
         | oil     | target   | enfuel   | low     | low        | NA       | f        | NA         |
         | oil     | anecdote | refinera | low     | low        | a4       | f4       | NA         |
       - Anecdote + statistics:
          | project | role     | business | valence | similarity | analysis | features | statistics |
          |---------+----------+----------+---------+------------+----------+----------+------------|
          | oil     | target   | enfuel   | high    | high       | NA       | f        | high       |
          | oil     | anecdote | refinera | high    | high       | a1       | f1       | high       |
          | oil     | target   | enfuel   | low     | high       | NA       | f        | high       |
          | oil     | anecdote | refinera | low     | high       | a2       | f2       | high       |
          | oil     | target   | enfuel   | high    | low        | NA       | f        | high       |
          | oil     | anecdote | refinera | high    | low        | a3       | f3       | high       |
          | oil     | target   | enfuel   | low     | low        | NA       | f        | high       |
          | oil     | anecdote | refinera | low     | low        | a4       | f4       | high       |
         - I think the target will always have high statistics
           - Or maybe just for low valence?
           - Yes, it depends on valence
           - Also valence is positive/negative, not high/low
       - Anecdote + statistics amended:
          | project | role     | business | valence  | similarity | analysis | features | statistics |
          |---------+----------+----------+----------+------------+----------+----------+------------|
          | oil     | target   | enfuel   | positive | high       | NA       | f        | low        |
          | oil     | anecdote | refinera | positive | high       | a1       | f1       | low        |
          | oil     | target   | enfuel   | negative | high       | NA       | f        | high       |
          | oil     | anecdote | refinera | negative | high       | a2       | f2       | high       |
          | oil     | target   | enfuel   | positive | low        | NA       | f        | low        |
          | oil     | anecdote | refinera | positive | low        | a3       | f3       | low        |
          | oil     | target   | enfuel   | negative | low        | NA       | f        | high       |
          | oil     | anecdote | refinera | negative | low        | a4       | f4       | high       |
       - Also, statistics isn't really relevant to the anecdote; only to target.
         - But again, it seems to be useful for filtering, even though they're duplicated.
       - Statistics only:
          | project | role     | business | valence | similarity | analysis | features | statistics |
          |---------+----------+----------+---------+------------+----------+----------+------------|
          | oil     | target   | enfuel   | NA      | NA         | NA       | f        | high       |
          | oil     | anecdote | NA       | NA      | NA         | NA       | NA       | high       |
     - So we make one of those for each domain.
     - Pair them up.
     - Then in each pair each one either acts as a target or a comparison each time.
       - I guess we do this through some filtering and latin unnesting
       - After you filter down to a condition, you get a target and anecdote row
         for each domain.
       - Each of those gets a value 1 or 2 for "target/comparison variation"
       - Or I guess just duplicate everything and given them a 1 and 2.
     - No just to figure out how to counterbalance each pair and their
       within-subjects condition
       - Surely just do the same thing as above.
       - Yeah, give each of the five conditions a vector of five for "project
         pair within-subject condition variation"
**** In action
     - For each project type
       - Anecdote condition
       - valence
         - role
         - business name
     - Maybe just go for it

* Aggregation
** Analysis
   - From Evan:
   #+begin_quote
   Ah right! Well I haven’t done that sort of thing before but I guess I’d
   probably start by looking at the number of alternations per 10 choices (is it
   a 2AFC type task?) or the average length of the run of the same choice (the
   two should be related of course)?
   #+end_quote

   - From Alex:
#+begin_quote
What you are describing makes sense though. You have less information in a
binary outcome than in a continuous or even a richer categorical outcome. So
it’s not possible to get a meaningful value for the autocorrelation if you don’t
have any information on how it varies, as in a sequence of all 1s or 0s. It’s a
similar issue to when you have a perfect predictor for a binary outcome. There
is no information in that predictor, because there is perfect separation, so you
have to exclude it from your model.

#+end_quote
* Alignment
** TODO Experiment 8
*** Plan  
     - Let's try work backwards:
     - Eventually we need to make a call to `trial_survey_multi_choice`, which creates timeline variables using `set_parameters`.
     - So each iteration of the loop should have a list of two for the two displays
     - I guess we can do everything in tibbles and then in the end nest the two displays (for reliability amount) in the end
       - And convert the two rows to a list
     - So everything can be on big tibble with the following variable columns:
       - Alignment condition
       - Reliability type
       - Project variation
         - Low alignment: intrinsic features
         - High alignment: project type
     - Then supposedly, filtering that down (and passing the relevant column value to display_if for condition) will get you the two displays for each combination
       - but actually it wouldn't be filtering, it would be stepping down each row and pulling the 'parameters'
     - Actually configuring the two reliability amount displays isn't that easy
       - They need different NPVs
       - Also, they need five new project descriptions
     - New issue
       - You ran Aggregation Experiment 3b (high alignment top up) with the Experiment 4 link.
       - So not the end of the world, but now what you probably need to do is to hard code those three IDs out of Experiment 4 prolific ID generation.
*** DONE Counterbalancing
    CLOSED: [2020-10-08 Thu 16:40]
    - Project variation
      - But this is randomised
    - Project name
      - Is it really just project name that needs latin square?
    - Also order of the values in each table, I guess.
      - Or maybe it's enough to just change the order of values, and we don't care so much about the order of the names
    - Ok so project name was definitely not as easy as I thought.
      - Maybe because of the alignment differences
      - So we're now going to try do it after everything
    - That worked fine
      - But might lead to errors when getting input data through, so pay attention.
    - And column order
    - We're currently on 12MB, so likely will need to revisit this and add column shuffling on the JS end
*** DONE Inputs
    CLOSED: [2020-10-09 Fri 11:50]
*** DONE Fix project aesthetics [5/5]
    CLOSED: [2020-11-03 Tue 09:50]
    - [X] Heading names in the final table
    - [X] Multiplier values
      - [X] Actual values
      - [X] Rounding
        - Done automatically due to the integer conversion
    - [NA] Allocation and ranking labels
      - Might be too hard to bother
      - Removed
      - The others didn't suggest to add this
    - [X] Business names
    - [X] Table width
    - [X] Add project type underline
*** DONE Make sure projects are different between displays
    CLOSED: [2020-10-12 Mon 18:13]
    - Different NPVs
    - Different projects for low alignment?
    - I guess you can do like in aggregation and sample pairs from the set of different projects
    - But what we can do is have "display A" and "display B"
      - And only five projects can appear in A and the other five in B
      - And we just randomise the order in jspsych
      - But then we're associating certain projects with low or high reliability
    - Instead we'd have to sample pairs like we said before
    - I guess it's a low vs high alignment issue again
      - With low alignment we can just have one of two variations
        - That is, either display A or B for low or high reliability
      - With high alignment that's where we might need to sample
      - Or not!
    - What if we just have a "project_display_variation" type of variable
      - And then within the variation of the display set, for high alignment you have the standard project_variation condition
      - So then I guess you're "merely" multiplying the conditions by two.
      - Add it at the beginning when adding project content
      - Then in the end nest the columns such that you get a tibble with a high and low reliability_amount column, a project_display_variation column that is `c(1,2)`, and the table contents
        - Supposedly then you'd have cases in which each display variation is associated with each reliability amount
*** DONE Figure out why ~materials_directory~ doesn't get rebuilt when testing is outdated.
    CLOSED: [2020-10-29 Thu 11:09]
    - Due to an upstream target not using the correct static branching map argument.
    - Specifically, ~testing~ had ~testing_directory~ as a dependency, and ~testing_directory~ had ~map(experiment_number)~ instead of ~.data = !!parameters~.
*** DONE Catch trials [2/2]
    CLOSED: [2020-10-17 Sat 12:51]
    - Can think of three types
      - Instructions check
      - Attention check
      - "Honesty check" (from [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6753310/#__sec25title][here]])
    - For the instructions check we can ask them which NPV is better
    - For attention check can be a trial between the two displays
      - Actually can be one before each display
      - "You will now see the first project display. It is important that you pay attention. Click the following checkbox before continuing on to the next page: [ ]. Please read through and complete the task accordingly."
    - Maybe also include a captcha?
      - Currently not working
      - Update: unlikely to happen, because requires update of psych server code
    - [[https://blog.prolific.co/how-to-improve-your-data-quality/][This article]] has a bunch of suggestions
      - And [[https://blog.prolific.co/minimising-noise-and-maximising-your-data-quality-the-case-of-satisficing/][here]]
    - Can also add something saying that you will get payment regardless of performance etc.
    - Let's reject if they get the NPV question and mid study attention check wrong
    - [X] Instructions check
    - [X] Attention check
*** DONE Generate the tables in JS
    CLOSED: [2020-10-10 Sat 15:43]
    - Will help with column order counterbalancing
      - And display pairs
    - So we just create the vectors/columns/whatever in R
      - Then jspsych takes them, shuffles, and puts them into a table from a function.
      - So we make a function whose argument is something like a vector of the columns/rows
        - And then the function itself already has the project and row names
        - The shuffling occurs in the function
    - So here's what you do:
      - Send to jspsych 1. an array of length five project columns, with each column as a vector (of 6 rows) in the array, 2. a vector for the header, 3. a vector for the row name column. For each table. Already in the function call.
      - Function steps:
        1. Shuffle the five vectors (columns)
        2. Add row name vector to the end of the array
        3. Transpose so that the rows are now columns
        4. Add header name vector
        5. Convert the new array of 6x6 into an HTML table
    - Would this help with display pairs?
      - Probably not actually
      - But what we can do is have "display A" and "display B"
        - And only five projects can appear in A and the other five in B
        - And we just randomise the order in jspsych
        - But then we're associating certain projects with low or high reliability
      - Instead we'd have to sample pairs like we said before
*** DONE Add reliability amount condition to input IDs
    CLOSED: [2020-10-17 Sat 13:38]
*** DONE Reanalyse old data using new techniques [3/3]
    CLOSED: [2020-11-03 Tue 09:50]
    - [X] difference between highest and lowest
      - Doesn't show an effect for Experiment 3 allocation
    - [X] mixed effect
      - Doesn't seem to work
      - Well, doesn't work when you try to play around with random effects
      - Works when you specify as in lm
      - Actually seems to be the best way to do this.
    - [X] covariate
      - but really just another within subject variable
      - But also: do we do project or npv amount?
        - Surely NPV amount
      - ANOVA or regression?
        - Seems equivalent
        - aov_ez doesn't seem to work with drake because character(0) isn't being taken as an argument
        - So we've got either aov_car or lm
          - Somehow lm seems to make more sense, because npv_amount is more continuous than categorical maybe?
        - Well how hard is it to do both?
          - Probably not that hard, but let's just start with lm
        - But now it seems that they're showing different estimates
        - So yeah let's do both
        - Ask informatics hub?
    - five regression
      - Asked Bruce to clarify
      - cancelled
*** DONE Add explanation of allocation task
    CLOSED: [2020-10-26 Mon 10:18]
    - Either in instructions or as preamble
*** DONE Generate test data
    CLOSED: [2020-10-22 Thu 17:48]
    - For some reason it isn't showing up with webdriver
    - But it has something to do with the main code, because welcome page works by itself
    - Also you changed around the experiment files for aggregation 4 and the resources for it
    - mock data files have also been edited a bit
**** DONE Add ad hoc webdriver code to satisfy ranking and allocation requirements [4/5]
     CLOSED: [2020-10-22 Thu 17:47]
     - [ ] Maybe add table class
       - Can also call "table", but probably better to use class in case we use different tables later
     - [X] Add ranking class
       - So that you can pull them out easier using webdriver
     - [X] Add allocation class
     - [X] Add ranking webdriver code
     - [X] Add allocation webdriver code
**** DONE Screenshots
     CLOSED: [2020-10-23 Fri 15:39]
*** TODO Add project number to input ID
*** TODO Change the NPV generation code a bit so that there isn't a duplicate value between sets
*** TODO Work out why you can pass symbols for functions in drake, but not for arguments
*** TODO Power analysis
    - Now that we're using ~lmer~ for analyses, we need to use something like
      ~simr~
    - What we can do is get previous experiments and then change the effect
      sizes as required.
    - I guess one issue is that we don't have a perfect pilot.
    - But we do have simulated data now.
    - Also we're meant to do sub component analyses?
      - Actually we're meant to do both to compare the sample needed
**** From http://finzi.psych.upenn.edu/R/library/simr/doc/fromscratch.html
         #+begin_src R
           library(simr)

           x <- 1:10
           g <- letters[1:3]
           X <- expand.grid(x = x, g = g)

           b <- c(2, -0.1) # fixed intercept and slope
           V1 <- 0.5 # random intercept variance
           V2 <- matrix(c(0.5, 0.05, 0.05, 0.1), 2) # random intercept and slope variance-covariance matrix
           s <- 1 # residual standard deviation

           model1 <- makeLmer(y ~ x + (1|g), fixef=b, VarCorr=V1, sigma=s, data=X)
           powerSim(model1, nsim=20)

         #+end_src
         - So I guess we just have to do it from scratch?
**** Test with already simulated data
     #+begin_src R
       library(drake)
       library(tidyverse)
       library(simr)

       loadd(data_simulation_alignment_8)

       formula <-
         allocation ~ alignment * reliability_amount * reliability_type * npv_amount + (1 | id)

       model <-
         formula %>%
         lmer(
           data = data_simulation_alignment_8
         )

       model %>%
         doTest(fcompare(~ alignment + reliability_amount))


       lm1 <- lmer(y ~ x + (x|g), data=simdata)
       lm0 <- lmer(y ~ x + (1|g), data=simdata)
       anova(lm1, lm0)
       compare(. ~ x + (1|g))(lm1)
       rcompare(~ (1|g))(lm1)
     #+end_src
**** More research
     - Some useful resources all related to @singmann2019:
       - https://stats.stackexchange.com/questions/130714/how-to-choose-random-and-fixed-effects-structure-in-linear-mixed-models
       - http://singmann.org/mixed-models-for-anova-designs-with-one-observation-per-unit-of-observation-and-cell-of-the-design/
       - https://cran.r-project.org/web/packages/afex/vignettes/afex_mixed_example.html
**** So let's give it ago
     - Just following the afex::mixed vignette.
      #+begin_src R
        library(tidyverse)
        library(afex)
        library(drake)

        loadd(data_clean_alignment_8)

        ## look normal as is
        data_clean_alignment_8 %>% 
          mutate(
            log_allocation = log(allocation)
          ) %>%
          pivot_longer(cols = c(allocation, log_allocation),
                       names_to = "allocation_type",
                       values_to = "allocation") %>%
        ggplot(aes(allocation)) +
          geom_histogram(bins = 100) +
          facet_wrap(vars(allocation_type), scales = "free_x")

        model1 <-
          data_clean_alignment_8 %>% 
          mixed(
            allocation ~ alignment * reliability_type * reliability_amount * npv_amount + (alignment * reliability_type * reliability_amount * npv_amount | id),
            data = .
          )

        ## Warning messages:
        ## 1: Model failed to converge with 28 negative eigenvalues: -2.0e-01 -3.1e-01 -9.3e-01 -2.2e+00 -4.2e+00 -7.7e+00 -1.2e+01 -8.0e+01 -4.9e+02 -8.3e+02 -1.1e+03 -2.3e+03 -2.9e+03 -3.4e+03 -4.3e+03 -8.6e+03 -1.2e+04 -1.9e+04 -2.5e+04 -3.0e+04 -3.3e+04 -4.3e+04 -4.7e+04 -6.6e+04 -4.2e+05 -1.3e+06 -2.1e+06 -3.4e+06
        ## 2: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 3: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 4: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 5: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 6: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 7: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 8: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 9: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 10: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 11: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 12: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 13: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 14: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 15: Unable to compute Kenward-Roger F-test: using Satterthwaite instead
        ## 16: Unable to compute Kenward-Roger F-test: using Satterthwaite instead

        ## Warning message:
        ##           lme4 reported (at least) the following warnings for 'full':
        ##                                                                 * boundary (singular) fit: see ?isSingular 

        summary(model1)$varcor
      #+end_src
**** Action plan
     - Seems like we're going to have to do the standard simulate, analyse, and iterate.
     - But how will we specify the effect sizes?
     - At the moment we're simulating allocation using the correlation.
     - You know what else we can do
       - Just determine a mean for each NPV amount!
     - Unless there's a standard way to simulate these kinds of continuous
       variables
     - Usually you just use ~rnorm~ or one of the other distributions
     - Technically we should be working out what kind of distribution this is
     - We can also generate y by running the regression backwards, as [[https://stats.stackexchange.com/questions/115748/simulate-data-for-2-x-2-anova-with-interaction/115767][this
       suggests]].
       - I think we can just use ~lme4::simulate.merMod()~
     - [[https://aosmith.rbind.io/2018/04/23/simulate-simulate-part-2/][This one]] suggests that we can just add npv amount in as is and you'll get
       the response variable as long as you specify the others.
     - But [[https://github.com/RInterested/SIMULATIONS_and_PROOFS/blob/master/Trees%20mixed%20random%20effects][this]] suggests more than one fixed factor might be more involved.
***** DeBruine and Barr (2019)
      - I think [[https://debruine.github.io/lmem_sim/articles/paper.html][this]] is a good guide, because it's recent and peer reviewed
      - Or we use her [[https://github.com/debruine/faux][faux]] package
      - DeBruine and Barr (2019) suggest that SR approximation is actually better
        for lmer's REML, and cite Luke (2017).
      - They also have an interesting mention of what to do without pilot data:
      #+begin_quote
       If you lack any pilot data to work with, you can start with the general
       rule of thumb setting the residual variance to about twice the size of the
       by-subject or by-item variance components (see supplementary materials
       from Barr et al., 2013 at
       https://talklab.psy.gla.ac.uk/simgen/realdata.html for results from an
       informal convenience sample).
      #+end_quote
      - Barr et al. (2013) also has some practical tips.
*****  [[https://debruine.github.io/tutorials/sim-lmer.html][DeBruine's sim-lmer tutorial]]
      - Also useful
      - Goes into interactions
        - But of categorical variables
        - And suggests to set the values in relation to the grand mean
        - Transforms them into main effects and interactions
      - Slopes
        - Suggests to only add within-subjects factors
        - Contrary to [[https://stats.stackexchange.com/a/408983][this example]].
        - But I guess we'll go with DeBruine?
        - Also just categorical
        - [[https://stats.stackexchange.com/a/162735][This]] is an example of using time, which is isimilar to NPV amount
***** What does {faux} have to offer?
      - Ideally it automates a lot of this stuff
      - Otherwise we'll do as per the tutorial
      - Yeah looks like it's too specific to designs with item random effects.
      - Unless maybe we can use ~sim_design()~?
        - Doesn't seem possible/simple
**** Actual action plan
     - Bring in alignment experiments 2, 3, and 7.
       - 2 has information about NPV amount, reliability amount, and alignment
         for explicit reliability,
       - 3 has information about NPV amount, reliability amount, and alignment
         for implicit reliability,
         - Although these results are different to 7
         - Here there was a main effect of alignment
         - So maybe let's not include 3
       - 7 has information about reliability amount and reliability type for
         alignment condition separately.
     - Model
       #+begin_src R
                  allocation ~
                    alignment * reliability_type * reliability_amount * npv_amount +
                    (npv_amount * reliability_amount | id)
       #+end_src
     - Take relevant effects
       - 2
         - alignment fixed for explicit reliability
         - reliability amount fixed for explicit reliability
         - NPV amount fixed for explicit reliability
         - Subject variability
         - Subject x alignment correlation
         - Subject x reliability amount correlation
       - 7
     - Ok, let's just go for it
       
* Emacs
** DONE Spotify
   CLOSED: [2020-10-31 Sat 15:03]
** TODO Email
** DONE combine ESS configs
   CLOSED: [2020-10-20 Tue 10:05]
** TODO Work out how to work with the additional ESS config
   - That is, do we keep it in the layer, or user-config, or private layer?
** DONE Elaborate on syntax highlighting question
  CLOSED: [2020-10-03 Sat 19:00]
** TODO Figure out listviewer
** TODO Figure out ess-r-view-data
** DONE Line wrap without breaking up words
   CLOSED: [2020-11-03 Tue 09:40]
   - visual-line-mode
** TODO Evernote
** DONE Automatically start R REPL in project root
   CLOSED: [2020-10-20 Tue 10:08]
   - Looks like it's a feature-not-bug situation
   - You want to be asked, because otherwise there isn't an easy way of determining
   - RStudio has their .Rproj files, but it seems ESS doesn't want to do that
** TODO Get graphics device in buffer
** TODO Format while typing
* Meeting notes
** Micah, Dan, Bruce <2020-11-02 Mon> 
*** Alignment 8
    - Micah
      - Overall good
**** DONE Add explanation for explicit reliability 
     CLOSED: [2020-11-02 Mon 19:14]
      - But: way I call NPV unreliable
        - the implication of that: unreliable because estimates are imperceise
     - precision of the metric prediction
     - Also:
     #+begin_quote
     I would also change "in this particular industry, NPV is an unrelaible
predictor of project success" to "in this industry, NPV is an unreliable
predictor of a project's profits"
     #+end_quote
     - Unreliability
       - Good feature some times too
       - Usually it means that there's a greater upside and downside
       - Makes it sound like a bad thing. Whereas when implicitly unreliable
         might be different
         - Unreliable because it's such a wide range of outcomes
           - broader range = meaning we have less confidence
**** Four way interaction
       - Focus on subcomponents
       - If hypotheses only concern a subset
       - Three way interaction for explicit, and no for implicit
         - implies four-way
       - how much more expensive
         - four way is right way to go, other wise have to predict no effect
       - can specify the interactions
**** TODO power analysis
**** Instructions
     - NPV check
       - Maybe part of experiment
       - On a simple task
       - maybe forcing NPV down their throats
       - But actually ok
*** Aggregation
**** DONE Send aggregation analyses
     CLOSED: [2020-11-02 Mon 19:14]
       - Thursday - just about aggregation
      
   
